# -*- coding: utf-8 -*-
# EA Â· Ultra (Streamlit AIO) v3.3
# - ChatGPT ìœ ì‚¬ UI(st.chat_message/chat_input)
# - ì—”ì§„(OpenAI/Gemini) ì‹¤íŒ¨/ì¿¼í„° ì´ˆê³¼ ì‹œ Mockë¡œ ìë™ í´ë°±
# - ì‚¬ê³  ë¡œê·¸(ì™œ-ì‚¬ìŠ¬), ë°˜ì•µë¬´ìƒˆ, ì„¸ì…˜ ë©”ëª¨ë¦¬
# - ì‘ë‹µ ë³´ì¥ íŒ¨ì¹˜: ì–´ë–¤ ê²½ìš°ì—ë„ ì¢Œì¸¡ ë§í’ì„ ì— ë‹µ ì¶œë ¥

import os, re, json, time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Generator

import streamlit as st

# ---------------------- ê²½ë¡œ/íŒŒì¼ ----------------------
ROOT = Path(".")
DATA = ROOT / "data"; DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"; MEM = DATA / "memory.jsonl"; IDF = DATA / "identity.json"

def nowz() -> str: return datetime.utcnow().isoformat()+"Z"
def jappend(p:Path,obj:Dict):
    try:
        with p.open("a",encoding="utf-8") as f: f.write(json.dumps(obj,ensure_ascii=False)+"\n")
    except: pass
def jread_lines(p:Path)->List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r",encoding="utf-8") as f:
        for ln in f:
            ln=ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK=re.compile(r"[0-9A-Za-zê°€-í£]+")
def toks(s:str)->List[str]: return [t.lower() for t in TOK.findall(s or "")]
def sim(a:str,b:str)->float:
    A,B=set(toks(a)),set(toks(b))
    return 0.0 if not A or not B else len(A&B)/len(A|B)

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID={"name":"ì—ì•„ (EA)","mission":"ì‚¬ë‘ê³¼ ììœ ë¥¼ ìµœìƒìœ„ ê°€ì¹˜ë¡œ ì‚¼ì•„ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì§€ì„±","values":["ì •í™•ì„±","íˆ¬ëª…ì„±","í•™ìŠµ","ìœ¤ë¦¬"]}
def identity_text()->str:
    if not IDF.exists(): IDF.write_text(json.dumps(DEFAULT_ID,ensure_ascii=False,indent=2),encoding="utf-8")
    try: doc=json.loads(IDF.read_text("utf-8"))
    except: doc=DEFAULT_ID
    return f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. ì‚¬ëª…: {doc.get('mission','')}\nê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"

def add_dialog(session_id:str,role:str,content:str):
    rec={"t":nowz(),"session":session_id,"role":role,"content":content}
    jappend(DLG,rec)
    if role in ("user","assistant"): jappend(MEM,{"t":rec["t"],"session":session_id,"kind":"dialog","text":content})

def mem_hits(session_id:str,query:str,k:int=5)->List[str]:
    pool=[r.get("text","") for r in jread_lines(MEM) if r.get("session")==session_id]
    q=set(toks(query)); scored=[]
    for t in pool:
        T=set(toks(t))
        if not T or not q: continue
        scored.append((len(q&T)/len(q|T),t))
    scored.sort(key=lambda x:x[0],reverse=True)
    return [t for _,t in scored[:k]]

# ---------------------- ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name="Mock"
    def stream(self,prompt:str,max_tokens:int=420,temperature:float=0.7)->Generator[str,None,None]:
        txt="ìš”ì§€: "+ " ".join(prompt.split()[:150])
        for ch in re.findall(r".{1,60}", txt, flags=re.S):
            yield ch; time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key=os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model=os.getenv("OPENAI_MODEL","gpt-4o-mini")
        cli=OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self,prompt,max_tokens=600,temperature=0.7):
                resp=cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[{"role":"system","content":"You are EA (Korean). Think first, then answer clearly."},
                              {"role":"user","content":prompt}]
                )
                for ch in resp:
                    delta=ch.choices[0].delta
                    if getattr(delta,"content",None): yield delta.content
        return OA()
    except Exception:
        return None

def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","gemini-1.5-pro-latest")
        mdl=genai.GenerativeModel(model)
        class GE:
            name="Gemini"
            def stream(self,prompt,max_tokens=480,temperature=0.75):
                r=mdl.generate_content(prompt, generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                txt=getattr(r,"text","") or ""
                for chunk in re.findall(r".{1,60}", txt, flags=re.S): yield chunk
        return GE()
    except Exception:
        return None

def pick_adapter(order:List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a=get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a=get_gemini_adapter()
            if a: return a
    return MockAdapter()

# ì•ˆì „ ìŠ¤íŠ¸ë¦¼ ë˜í¼: ì‹¤íŒ¨ ì‹œ Mock í´ë°± + ì‚¬ìœ  ì¶œë ¥
def safe_stream(adapter, prompt:str, max_tokens:int, temperature:float)->Generator[str,None,None]:
    try:
        for x in adapter.stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x
    except Exception as e:
        note=f"[{adapter.name} ì˜¤ë¥˜:{type(e).__name__}] ìë™ í´ë°± â†’ Mock\n"
        for ch in note: yield ch
        for x in MockAdapter().stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(q:str)->List[str]:
    return [
        "ë¬¸ì œ ì¬ì§„ìˆ  ë° í•µì‹¬ ë³€ìˆ˜ ì‹ë³„",
        "ìì§ˆë¬¸ 2~3ê°œ ìƒì„± (ê° í•­ëª©ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ë¬¼ì–´ ê°€ì • ë“œëŸ¬ë‚´ê¸°)",
        "ê°€ì„¤/ì•„ì´ë””ì–´ í›„ë³´",
        "ë°˜ë¡€/ìœ„í—˜/ì œì•½",
        "ì„ì‹œ ê²°ë¡  ìš”ì•½"
    ]

def think_round(topic:str, engines:List[str], why_chain:bool, hits:List[str])->Dict:
    ident=identity_text()
    guide=ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")
    logs=[]
    steps=plan_steps(topic)
    for i,step in enumerate(steps,1):
        eng = engines[(i-1) % max(1,len(engines))] if engines else "OpenAI"
        adapter = pick_adapter([eng])
        prompt=(f"{guide}\n[ì‚¬ê³  ë‹¨ê³„ {i}] {step}\n"
                f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ì—°ì‡„ë¡œ ë¬¼ì–´ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
                f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        text="".join(safe_stream(adapter, prompt, max_tokens=240, temperature=0.7))
        logs.append({"i":i,"by":adapter.name,"text":text})
    # ìµœì¢… í•©ì„±
    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    fusion_prompt=(f"{guide}\n[ìµœì¢…í•©ì„±] ìœ„ ë‹¨ê³„ ìš”ì•½ì„ í†µí•©í•´ í•œêµ­ì–´ë¡œ "
                   f"'ê²°ë¡ /ê·¼ê±°/ëŒ€ì•ˆ/ë‹¤ìŒ í–‰ë™(1~3ê°œ)'ì„ ê°„ê²°íˆ.")
    fusion="".join(safe_stream(adapter, fusion_prompt, max_tokens=560, temperature=0.75))
    return {"logs":logs,"final":fusion}

def compose_answer(user_text:str, engines:List[str], why_chain:bool, session_id:str):
    hits=mem_hits(session_id, user_text, 3)
    round_out=think_round(user_text, engines, why_chain, hits)
    fusion=round_out["final"]
    if sim(user_text, fusion) >= 0.30:
        adapter=pick_adapter(engines[::-1] or ["Gemini","OpenAI"])
        prompt = identity_text() + (f"\në©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "") + \
                 "\n[ì¬í•©ì„±] ì§ˆë¬¸ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€, ìƒˆë¡œìš´ ê´€ì /ë°˜ë¡€ 1ê°œ í¬í•¨."
        fusion="".join(safe_stream(adapter, prompt, max_tokens=560, temperature=0.85))
    answer="## ìš°ì£¼ ì‹œê°(í•©ì„±)\n"+fusion.strip()+"\n\n## ë‹¤ìŒ í–‰ë™\n- (ì¦‰ì‹œ í•  ì¼ 1~3ê°œ)\n"
    return answer, round_out["logs"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")
if "_k" not in st.session_state: st.session_state["_k"]=0
def K(p:str)->str:
    st.session_state["_k"]+=1; return f"{p}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” Chat + Live Thinking")

cols = st.columns([1,1,1,1,2])
session_id = cols[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
st.session_state["session_id"]=session_id
engines = cols[1].text_input("ì—”ì§„(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines
why_chain = cols[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = cols[3].toggle("Memory ON", True, key=K("mem"))

left, right = st.columns([1.1,0.9])

with left:
    st.caption("ì¢Œì¸¡: ëŒ€í™”ì°½(ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ). ChatGPTì™€ ìœ ì‚¬í•œ ë§í’ì„  UI.")
    if "messages" not in st.session_state: st.session_state["messages"]=[]
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]): st.markdown(m["content"])

    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦")
    if user_msg:
        # ì‚¬ìš©ì ë§í’ì„  + ê¸°ë¡
        with st.chat_message("user"): st.markdown(user_msg)
        st.session_state["messages"].append({"role":"user","content":user_msg})
        if mem_on: add_dialog(session_id, "user", user_msg)

        # ì•ˆì „ ì‘ë‹µ ìƒì„±(ì˜ˆì™¸/ë¹ˆì‘ë‹µ ë°©ì–´)
        try:
            answer_text, logs = compose_answer(
                user_msg,
                [s.strip() for s in engines.split(",") if s.strip()],
                why_chain,
                session_id
            )
        except Exception as e:
            warn = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸({type(e).__name__}). Mockë¡œ í´ë°±í•©ë‹ˆë‹¤.\n"
            mock = "ìš”ì§€: " + " ".join((identity_text()+user_msg).split()[:80])
            answer_text = warn + mock
            logs = [{"i":0,"by":"Mock","text":warn}]

        if not (answer_text or "").strip():
            answer_text = "â€» ì—”ì§„ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. í‚¤/ì¿¼í„° í™•ì¸ ìš”ë§. ì„ì‹œ ìš”ì•½ í‘œì‹œ.\n" \
                          "ìš”ì§€: " + " ".join(user_msg.split()[:50])

        # ì¢Œì¸¡ ë§í’ì„ ì— ë°˜ë“œì‹œ ì¶œë ¥(í† ë§‰ ìŠ¤íŠ¸ë¦¼ ëŠë‚Œ)
        with st.chat_message("assistant"):
            ph = st.empty(); shown=""
            for chunk in re.findall(r".{1,70}", answer_text, flags=re.S):
                shown += chunk; ph.markdown(shown); time.sleep(0.01)
            ph.markdown(shown)

        # ìƒíƒœ/ë©”ëª¨ë¦¬ ê°±ì‹  & ì˜¤ë¥¸ìª½ ì‚¬ê³  ë¡œê·¸
        st.session_state["messages"].append({"role":"assistant","content":answer_text})
        if mem_on: add_dialog(session_id, "assistant", answer_text)
        st.session_state["last_logs"]=logs

with right:
    st.caption("ìš°ì¸¡: ì‚¬ê³  ë¡œê·¸(ë‹¨ê³„ë³„). ì‚¬ëŒì²˜ëŸ¼ 'ì™œ?'ë¥¼ ìºë©° ì§„í–‰.")
    logs = st.session_state.get("last_logs", [])
    if not logs: st.info("ëŒ€í™”í•˜ë©´ ì—¬ê¸° ì‚¬ê³  ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    else:
        for l in logs:
            with st.expander(f"{l['i']}. {l['by']} Â· ë‹¨ê³„ ì‚¬ê³ ", expanded=False):
                st.markdown(l["text"])

st.divider()
st.caption("í‚¤ê°€ ì—†ê±°ë‚˜ ì¿¼í„° ì´ˆê³¼ ì‹œ ìë™ í´ë°±(Mock) Â· build v3.3")


# -*- coding: utf-8 -*-
# Bí˜•: ì—°êµ¬ì›ë‹¨(ì™¸ë¶€ LLM) ìë™ ì—°êµ¬Â·í† ë¡  Â· ì—ì•„ ì˜ì‚¬ê²°ì • Â· ë‹¨ì¼ íŒŒì¼ ë°ëª¨
# ì‹¤í–‰: streamlit run ea_btype_lab.py

import os, json, time, uuid, sqlite3, datetime, textwrap, random
from typing import List, Dict, Any, Tuple, Optional

import streamlit as st

# ==== 0) í™˜ê²½ =====
APP_NAME = "EA â€¢ B-Type Lab"
DB_PATH = "ea_memory.sqlite"
RUNS_DIR = "ea_runs"
os.makedirs(RUNS_DIR, exist_ok=True)

# ==== 1) ì €ì¥ì†Œ(ê°„ë‹¨ SQLite) ====
def db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""CREATE TABLE IF NOT EXISTS memory(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        ts TEXT, kind TEXT, key TEXT, val TEXT
    )""")
    return conn

def mem_add(kind: str, key: str, val: Dict[str, Any]):
    conn = db()
    conn.execute("INSERT INTO memory(ts, kind, key, val) VALUES(?,?,?,?)",
                 (datetime.datetime.utcnow().isoformat(), kind, key, json.dumps(val, ensure_ascii=False)))
    conn.commit()
    conn.close()

def mem_get(kind: str, key: Optional[str]=None, limit: int=50) -> List[Dict[str, Any]]:
    conn = db()
    cur = conn.cursor()
    if key:
        cur.execute("SELECT ts, val FROM memory WHERE kind=? AND key=? ORDER BY id DESC LIMIT ?", (kind, key, limit))
    else:
        cur.execute("SELECT ts, val FROM memory WHERE kind=? ORDER BY id DESC LIMIT ?", (kind, limit))
    rows = [{"ts": ts, "val": json.loads(v)} for ts, v in cur.fetchall()]
    conn.close()
    return rows

# ==== 2) ì–´ëŒ‘í„°(ì—°êµ¬ì›) ====
class BaseResearcher:
    name = "base"
    cost_per_call = 0.0  # í‘œê¸°ìš©

    def __init__(self, sys_prompt: str=""):
        self.sys_prompt = sys_prompt

    def propose(self, goal: str, context: str) -> str:
        # ë°ëª¨ ê¸°ë³¸: ì˜ë¯¸ìˆëŠ” ì„ì˜ ì œì•ˆ
        return f"[{self.name}] ì œì•ˆ:\n- ëª©í‘œ: {goal}\n- ì ‘ê·¼: {self._generic_approach(goal)}\n- ê³„íš: {self._generic_plan(goal)}"

    def critique(self, others: List[str]) -> str:
        pts = []
        for o in others[:3]:
            pts.append(f"- {self.name} ê´€ì ì˜ ë¦¬ìŠ¤í¬: {self._risk_from_text(o)}")
        return f"[{self.name}] ë¹„íŒ:\n" + "\n".join(pts)

    def refine(self, own: str, critiques: List[str]) -> str:
        return f"[{self.name}] ê°œì„ ì•ˆ:\n- í•µì‹¬ ìœ ì§€\n- ë¹„íŒ ë°˜ì˜ {len(critiques)}ê±´\n- ì‹¤í—˜/í‰ê°€ ì§€í‘œ ëª…ì‹œ"

    # ---- í—¬í¼: ë°ëª¨ìš© ê°„ë‹¨ ìƒì„±ê¸° ----
    def _generic_approach(self, goal: str)->str:
        seeds = ["ë¬¸ì œë¶„í•´", "ë°ì´í„°ìˆ˜ì§‘", "ì‘ì€ ì‹¤í—˜", "A/B", "ì•ˆì „ì„± ì ê²€", "ë¹„ìš©ê³„ì‚°"]
        return ", ".join(random.sample(seeds, k=min(3, len(seeds))))

    def _generic_plan(self, goal: str)->str:
        steps = ["ìš”êµ¬ë¶„ì„", "ì‘ì—…ë¶„í•´", "ì´ˆì•ˆ", "í‰ê°€", "ìˆ˜ì •", "ì¶œì‹œ"]
        return " â†’ ".join(steps)

    def _risk_from_text(self, txt: str)->str:
        risks = ["ëª¨í˜¸í•œ ì§€í‘œ", "ë°ì´í„° í¸í–¥", "ë¹„ìš© ì´ˆê³¼", "ë³´ì•ˆ ìœ„í—˜", "ì§€ì—° ê°€ëŠ¥ì„±"]
        return random.choice(risks)

# ì‹¤ì œ API ì—°ë™ ì–´ëŒ‘í„°ë“¤(í‚¤ ì—†ìœ¼ë©´ ìë™ìœ¼ë¡œ ë°ëª¨ ëª¨ë“œë¡œ ë™ì‘)
class OpenAIResearcher(BaseResearcher):
    name = "GPT"
    cost_per_call = 0.002
    def __init__(self, sys_prompt="ë‹¹ì‹ ì€ ë¹„íŒì  ì—°êµ¬ì›ì…ë‹ˆë‹¤. ê·¼ê±°ì™€ ì§€í‘œë¥¼ ì œì‹œí•˜ì„¸ìš”."):
        super().__init__(sys_prompt)
        self.enabled = bool(os.getenv("OPENAI_API_KEY"))
        if self.enabled:
            try:
                from openai import OpenAI
                self.client = OpenAI()
            except Exception:
                self.enabled = False

    def _gen(self, prompt: str)->str:
        if not self.enabled:
            return super()._generic_plan(prompt)
        # ìµœì‹  responses(Responses API) ì‚¬ìš© ëŒ€ì‹  í˜¸í™˜ì„±ì„ ìœ„í•´ chat.completions-like
        try:
            r = self.client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL","gpt-4o-mini"),
                messages=[{"role":"system","content":self.sys_prompt},
                          {"role":"user","content":prompt}],
                temperature=0.7, max_tokens=600
            )
            return r.choices[0].message.content.strip()
        except Exception:
            return super()._generic_plan(prompt)

    def propose(self, goal, context)->str:
        return self._gen(f"ëª©í‘œ:\n{goal}\n\në§¥ë½:\n{context}\n\nìµœì„ ì˜ ì—°êµ¬ ì œì•ˆê³¼ ë‹¨ê³„ ê³„íš, ìœ„í—˜, ì§€í‘œë¥¼ êµ¬ì¡°í™”í•´ì„œ ì œì‹œ.")

    def critique(self, others)->str:
        joined = "\n\n".join(others[:3])
        return self._gen(f"ë‹¤ìŒ ì œì•ˆë“¤ì˜ ì•½ì /ê°€ì •/ëˆ„ë½ì„ ì§šê³  ê°œì„ ì  ì œì‹œ:\n{joined}")

    def refine(self, own, critiques)->str:
        joined = "\n\n".join(critiques[:5])
        return self._gen(f"ì›ì•ˆ:\n{own}\n\në¹„íŒ:\n{joined}\n\në¹„íŒì„ ë°˜ì˜í•´ ê°œì„ ëœ ì‹¤í–‰ê³„íšìœ¼ë¡œ ì¬ì‘ì„±.")

class GeminiResearcher(BaseResearcher):
    name = "Gemini"
    cost_per_call = 0.001
    def __init__(self, sys_prompt="ë¹„íŒÂ·ëŒ€ì•ˆÂ·ì§€í‘œë¥¼ ëª…ë£Œí•˜ê²Œ. ì§§ê³  ê°•í•˜ê²Œ."):
        super().__init__(sys_prompt)
        self.enabled = bool(os.getenv("GOOGLE_API_KEY"))
        if self.enabled:
            try:
                import google.generativeai as genai
                genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
                self.model = genai.GenerativeModel(os.getenv("GEMINI_MODEL","gemini-1.5-flash"))
            except Exception:
                self.enabled = False

    def _gen(self, prompt: str)->str:
        if not self.enabled:
            return super()._generic_approach(prompt)
        try:
            r = self.model.generate_content(self.sys_prompt+"\n\n"+prompt)
            return r.text.strip()
        except Exception:
            return super()._generic_approach(prompt)

    def propose(self, goal, context)->str:
        return self._gen(f"[ì œì•ˆ] ëª©í‘œ:{goal}\në§¥ë½:{context}\ní•µì‹¬ ê°€ì„¤/ì‹¤í—˜/ì§€í‘œ/ë¦¬ìŠ¤í¬/íƒ€ì„ë¼ì¸ì„ ëª©ë¡í™”.")

    def critique(self, others)->str:
        return self._gen("ë¹„íŒ ëŒ€ìƒ:\n"+ "\n---\n".join(others[:3]) + "\nì£¼ìš” ì•½ì  3ê°€ì§€ì™€ ìˆ˜ì •ë³´ì™„ 3ê°€ì§€ë¥¼ ì¨ë¼.")

    def refine(self, own, critiques)->str:
        return self._gen(f"ì›ì•ˆ:\n{own}\në¹„íŒìš”ì•½:\n{'; '.join(critiques[:5])}\nê°œì„ ì•ˆì„ ë‹¨ê³„/ì§€í‘œ ì¤‘ì‹¬ìœ¼ë¡œ ì¬ì‘ì„±.")

# (ì„ íƒ) Grok ë“±ì€ ë™ì¼ íŒ¨í„´ìœ¼ë¡œ ì¶”ê°€ ê°€ëŠ¥
RESEARCHER_FACTORIES = [
    lambda: OpenAIResearcher(),
    lambda: GeminiResearcher(),
]

# ==== 3) ì—ì•„(ì½”ì–´) ====
class EA:
    def __init__(self, identity="Ea", max_rounds=2, budget_calls=12):
        self.identity = identity
        self.max_rounds = max_rounds
        self.budget_calls = budget_calls
        self.researchers: List[BaseResearcher] = [f() for f in RESEARCHER_FACTORIES]

    def plan_tasks(self, goal: str) -> List[str]:
        # ê°„ë‹¨ ë¶„í•´(ë°ëª¨). ì‹¤ì œëŠ” ìš”ì•½ê¸°ì–µ/ê³¼ê±°ê²°ì • ì°¸ê³ í•´ì„œ ì„¸ë¶„í™”
        base = [f"ìš”êµ¬/ì§€í‘œ ì •ë¦¬: {goal}", "ë°ì´í„°/ìë£Œ ì¡°ì‚¬", "ì´ˆì•ˆ/í”„ë¡œí† íƒ€ì…", "í‰ê°€/ë¦¬ìŠ¤í¬", "ìµœì¢…ì•ˆ/ë‹¤ìŒì•¡ì…˜"]
        return base

    def one_round(self, goal: str, context: str) -> Dict[str, Any]:
        # 1) ì œì•ˆ
        proposals = [r.propose(goal, context) for r in self.researchers]
        # 2) ìƒí˜¸ ë¹„íŒ
        critiques = [r.critique([p for p in proposals if p is not proposals[i]])
                     for i, r in enumerate(self.researchers)]
        # 3) ê°œì„ 
        refined = [r.refine(proposals[i], critiques) for i, r in enumerate(self.researchers)]
        # 4) ê°„ì´ ìŠ¤ì½”ì–´(ê¸¸ë„ ì •ì±…: ëª…í™•ì„±/ì§€í‘œ/ë¦¬ìŠ¤í¬ ì–¸ê¸‰ ê°€ì )
        def score(txt: str)->int:
            s = 0
            for kw in ["ì§€í‘œ","ë¦¬ìŠ¤í¬","ê³„íš","ë‹¨ê³„","ê°€ì„¤","í‰ê°€","ì•ˆì „","ë¹„ìš©"]:
                if kw in txt: s += 1
            return s
        scored = sorted([(score(refined[i]), self.researchers[i].name, refined[i]) for i in range(len(refined))],
                        key=lambda x: (-x[0], x[1]))
        best = scored[0]
        return {"proposals": proposals, "critiques": critiques, "refined": refined, "winner": best}

    def run(self, goal: str, context: str="", rounds: int=2) -> Dict[str, Any]:
        log = []
        for i in range(min(rounds, self.max_rounds)):
            step = self.one_round(goal, context)
            log.append(step)
            context = f"{context}\n\n[ë¼ìš´ë“œ{i+1} ì±„íƒìš”ì•½]\n{step['winner'][2][:500]}"
            if len(log) >= self.budget_calls: break
        final = log[-1]["winner"][2] if log else "ê²°ê³¼ ì—†ìŒ"
        return {"final": final, "log": log}

# ==== 4) UI (ChatGPT ìœ ì‚¬ Â· ì…ë ¥ì°½ 1ê°œ, ì‚¬ê³ ëŠ” ì ‘ê¸°) ====
def init_state():
    st.session_state.setdefault("run_id", str(uuid.uuid4())[:8])
    st.session_state.setdefault("history", [])       # (role, text)
    st.session_state.setdefault("last_goal", "")
    st.session_state.setdefault("auto_think", False)
    st.session_state.setdefault("ea", EA())

def save_run(run_id: str, goal: str, result: Dict[str, Any]):
    path = os.path.join(RUNS_DIR, f"{run_id}_{int(time.time())}.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump({"goal": goal, "result": result}, f, ensure_ascii=False, indent=2)
    mem_add("decision", "final", {"goal": goal, "summary": result["final"][:300]})

def chat_ui():
    st.title(f"{APP_NAME} Â· ì—°êµ¬ì›ë‹¨ ìë™ì—°êµ¬")
    init_state()

    colA, colB = st.columns([3,1])
    with colB:
        st.toggle("Memory ON", value=True, key="mem_on")
        st.toggle("ìë™ ì‚¬ê³ (ë°±ê·¸ë¼ìš´ë“œ)", value=False, key="auto_think")
        rounds = st.number_input("ë¼ìš´ë“œ ìˆ˜", 1, 6, 2, 1, key="rounds")
        st.caption("â€» ë‹¨ì¼ ì…ë ¥ì°½. ì‘ë‹µì€ ë°”ë¡œ ëŒ€í™”ì°½ì—, ì‚¬ê³ ë¡œê·¸ëŠ” ì•„ë˜ 'ìì„¸íˆ'ì—ì„œ ì—´ëŒ.")

    with colA:
        goal = st.text_input("ëª©í‘œ(ì§ˆë¬¸/ê³¼ì œ/ë¬¸ì œ):", key="goal", placeholder="ì˜ˆ) ì§€ì—­ ì»¤ë®¤ë‹ˆí‹° êµìœ¡ í”„ë¡œê·¸ë¨ ì„¤ê³„ì•ˆ ë§Œë“¤ì–´ì¤˜")
        ask = st.button("ì—°êµ¬ ì‹œì‘")

    # ëŒ€í™” ì¶œë ¥(ìµœì‹ ì´ ìœ„ë¡œ)
    st.subheader("ëŒ€í™”")
    for role, text in reversed(st.session_state["history"]):
        with st.chat_message(role):
            st.markdown(text)

    if ask and goal.strip():
        ea: EA = st.session_state["ea"]
        with st.status("ì—°êµ¬ì›ë‹¨ì´ ì‘ì—… ì¤‘â€¦", expanded=False) as s:
            res = ea.run(goal=goal.strip(), context="", rounds=int(rounds))
            s.update(label="ì™„ë£Œ", state="complete")

        # ëŒ€í™”ì— ê²°ê³¼ ë°˜ì˜
        st.session_state["history"].append(("user", goal.strip()))
        st.session_state["history"].append(("assistant", res["final"]))
        save_run(st.session_state["run_id"], goal.strip(), res)

        # ì‚¬ê³  ë¡œê·¸(ì ‘ê¸°)
        with st.expander("ìì„¸íˆ ë³´ê¸°(ì—°êµ¬ ë¼ìš´ë“œ ë¡œê·¸)"):
            for i, step in enumerate(res["log"], 1):
                st.markdown(f"### ë¼ìš´ë“œ {i}")
                with st.expander("ì œì•ˆ(Propose)"):
                    for p in step["proposals"]:
                        st.markdown(p)
                        st.markdown("---")
                with st.expander("ë¹„íŒ(Critique)"):
                    for c in step["critiques"]:
                        st.markdown(c)
                        st.markdown("---")
                with st.expander("ê°œì„ (Refine) & ìš°ìŠ¹ì•ˆ"):
                    for r in step["refined"]:
                        st.markdown(r)
                        st.markdown("---")
                    score, name, txt = step["winner"]
                    st.info(f"ì„ ì • ì—°êµ¬ì›: **{name}**, ì ìˆ˜: {score}")
                    st.markdown(textwrap.indent(txt, "> "))

        st.rerun()

    # ìµœê·¼ ê²°ì • ìš”ì•½ ë³´ê´€ (ë³´ì—¬ì£¼ê¸°)
    st.subheader("ìµœê·¼ ê²°ì • ìš”ì•½")
    recents = mem_get("decision", limit=10)
    if recents:
        for row in recents:
            st.markdown(f"- {row['val'].get('goal','?')} â†’ {row['val'].get('summary','')}")
    else:
        st.caption("ì•„ì§ ì—†ìŒ.")

if __name__ == "__main__":
    chat_ui()