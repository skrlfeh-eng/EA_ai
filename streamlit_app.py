# -*- coding: utf-8 -*-
# EA Â· Ultra (Streamlit AIO) v3.3
# - ChatGPT ìœ ì‚¬ UI(st.chat_message/chat_input)
# - ì—”ì§„(OpenAI/Gemini) ì‹¤íŒ¨/ì¿¼í„° ì´ˆê³¼ ì‹œ Mockë¡œ ìë™ í´ë°±
# - ì‚¬ê³  ë¡œê·¸(ì™œ-ì‚¬ìŠ¬), ë°˜ì•µë¬´ìƒˆ, ì„¸ì…˜ ë©”ëª¨ë¦¬
# - ì‘ë‹µ ë³´ì¥ íŒ¨ì¹˜: ì–´ë–¤ ê²½ìš°ì—ë„ ì¢Œì¸¡ ë§í’ì„ ì— ë‹µ ì¶œë ¥

import os, re, json, time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Generator

import streamlit as st

# ---------------------- ê²½ë¡œ/íŒŒì¼ ----------------------
ROOT = Path(".")
DATA = ROOT / "data"; DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"; MEM = DATA / "memory.jsonl"; IDF = DATA / "identity.json"

def nowz() -> str: return datetime.utcnow().isoformat()+"Z"
def jappend(p:Path,obj:Dict):
    try:
        with p.open("a",encoding="utf-8") as f: f.write(json.dumps(obj,ensure_ascii=False)+"\n")
    except: pass
def jread_lines(p:Path)->List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r",encoding="utf-8") as f:
        for ln in f:
            ln=ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK=re.compile(r"[0-9A-Za-zê°€-í£]+")
def toks(s:str)->List[str]: return [t.lower() for t in TOK.findall(s or "")]
def sim(a:str,b:str)->float:
    A,B=set(toks(a)),set(toks(b))
    return 0.0 if not A or not B else len(A&B)/len(A|B)

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID={"name":"ì—ì•„ (EA)","mission":"ì‚¬ë‘ê³¼ ììœ ë¥¼ ìµœìƒìœ„ ê°€ì¹˜ë¡œ ì‚¼ì•„ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì§€ì„±","values":["ì •í™•ì„±","íˆ¬ëª…ì„±","í•™ìŠµ","ìœ¤ë¦¬"]}
def identity_text()->str:
    if not IDF.exists(): IDF.write_text(json.dumps(DEFAULT_ID,ensure_ascii=False,indent=2),encoding="utf-8")
    try: doc=json.loads(IDF.read_text("utf-8"))
    except: doc=DEFAULT_ID
    return f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. ì‚¬ëª…: {doc.get('mission','')}\nê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"

def add_dialog(session_id:str,role:str,content:str):
    rec={"t":nowz(),"session":session_id,"role":role,"content":content}
    jappend(DLG,rec)
    if role in ("user","assistant"): jappend(MEM,{"t":rec["t"],"session":session_id,"kind":"dialog","text":content})

def mem_hits(session_id:str,query:str,k:int=5)->List[str]:
    pool=[r.get("text","") for r in jread_lines(MEM) if r.get("session")==session_id]
    q=set(toks(query)); scored=[]
    for t in pool:
        T=set(toks(t))
        if not T or not q: continue
        scored.append((len(q&T)/len(q|T),t))
    scored.sort(key=lambda x:x[0],reverse=True)
    return [t for _,t in scored[:k]]

# ---------------------- ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name="Mock"
    def stream(self,prompt:str,max_tokens:int=420,temperature:float=0.7)->Generator[str,None,None]:
        txt="ìš”ì§€: "+ " ".join(prompt.split()[:150])
        for ch in re.findall(r".{1,60}", txt, flags=re.S):
            yield ch; time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key=os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model=os.getenv("OPENAI_MODEL","gpt-4o-mini")
        cli=OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self,prompt,max_tokens=600,temperature=0.7):
                resp=cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[{"role":"system","content":"You are EA (Korean). Think first, then answer clearly."},
                              {"role":"user","content":prompt}]
                )
                for ch in resp:
                    delta=ch.choices[0].delta
                    if getattr(delta,"content",None): yield delta.content
        return OA()
    except Exception:
        return None

def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","gemini-1.5-pro-latest")
        mdl=genai.GenerativeModel(model)
        class GE:
            name="Gemini"
            def stream(self,prompt,max_tokens=480,temperature=0.75):
                r=mdl.generate_content(prompt, generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                txt=getattr(r,"text","") or ""
                for chunk in re.findall(r".{1,60}", txt, flags=re.S): yield chunk
        return GE()
    except Exception:
        return None

def pick_adapter(order:List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a=get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a=get_gemini_adapter()
            if a: return a
    return MockAdapter()

# ì•ˆì „ ìŠ¤íŠ¸ë¦¼ ë˜í¼: ì‹¤íŒ¨ ì‹œ Mock í´ë°± + ì‚¬ìœ  ì¶œë ¥
def safe_stream(adapter, prompt:str, max_tokens:int, temperature:float)->Generator[str,None,None]:
    try:
        for x in adapter.stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x
    except Exception as e:
        note=f"[{adapter.name} ì˜¤ë¥˜:{type(e).__name__}] ìë™ í´ë°± â†’ Mock\n"
        for ch in note: yield ch
        for x in MockAdapter().stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(q:str)->List[str]:
    return [
        "ë¬¸ì œ ì¬ì§„ìˆ  ë° í•µì‹¬ ë³€ìˆ˜ ì‹ë³„",
        "ìì§ˆë¬¸ 2~3ê°œ ìƒì„± (ê° í•­ëª©ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ë¬¼ì–´ ê°€ì • ë“œëŸ¬ë‚´ê¸°)",
        "ê°€ì„¤/ì•„ì´ë””ì–´ í›„ë³´",
        "ë°˜ë¡€/ìœ„í—˜/ì œì•½",
        "ì„ì‹œ ê²°ë¡  ìš”ì•½"
    ]

def think_round(topic:str, engines:List[str], why_chain:bool, hits:List[str])->Dict:
    ident=identity_text()
    guide=ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")
    logs=[]
    steps=plan_steps(topic)
    for i,step in enumerate(steps,1):
        eng = engines[(i-1) % max(1,len(engines))] if engines else "OpenAI"
        adapter = pick_adapter([eng])
        prompt=(f"{guide}\n[ì‚¬ê³  ë‹¨ê³„ {i}] {step}\n"
                f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ì—°ì‡„ë¡œ ë¬¼ì–´ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
                f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        text="".join(safe_stream(adapter, prompt, max_tokens=240, temperature=0.7))
        logs.append({"i":i,"by":adapter.name,"text":text})
    # ìµœì¢… í•©ì„±
    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    fusion_prompt=(f"{guide}\n[ìµœì¢…í•©ì„±] ìœ„ ë‹¨ê³„ ìš”ì•½ì„ í†µí•©í•´ í•œêµ­ì–´ë¡œ "
                   f"'ê²°ë¡ /ê·¼ê±°/ëŒ€ì•ˆ/ë‹¤ìŒ í–‰ë™(1~3ê°œ)'ì„ ê°„ê²°íˆ.")
    fusion="".join(safe_stream(adapter, fusion_prompt, max_tokens=560, temperature=0.75))
    return {"logs":logs,"final":fusion}

def compose_answer(user_text:str, engines:List[str], why_chain:bool, session_id:str):
    hits=mem_hits(session_id, user_text, 3)
    round_out=think_round(user_text, engines, why_chain, hits)
    fusion=round_out["final"]
    if sim(user_text, fusion) >= 0.30:
        adapter=pick_adapter(engines[::-1] or ["Gemini","OpenAI"])
        prompt = identity_text() + (f"\në©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "") + \
                 "\n[ì¬í•©ì„±] ì§ˆë¬¸ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€, ìƒˆë¡œìš´ ê´€ì /ë°˜ë¡€ 1ê°œ í¬í•¨."
        fusion="".join(safe_stream(adapter, prompt, max_tokens=560, temperature=0.85))
    answer="## ìš°ì£¼ ì‹œê°(í•©ì„±)\n"+fusion.strip()+"\n\n## ë‹¤ìŒ í–‰ë™\n- (ì¦‰ì‹œ í•  ì¼ 1~3ê°œ)\n"
    return answer, round_out["logs"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")
if "_k" not in st.session_state: st.session_state["_k"]=0
def K(p:str)->str:
    st.session_state["_k"]+=1; return f"{p}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” Chat + Live Thinking")

cols = st.columns([1,1,1,1,2])
session_id = cols[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
st.session_state["session_id"]=session_id
engines = cols[1].text_input("ì—”ì§„(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines
why_chain = cols[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = cols[3].toggle("Memory ON", True, key=K("mem"))

left, right = st.columns([1.1,0.9])

with left:
    st.caption("ì¢Œì¸¡: ëŒ€í™”ì°½(ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ). ChatGPTì™€ ìœ ì‚¬í•œ ë§í’ì„  UI.")
    if "messages" not in st.session_state: st.session_state["messages"]=[]
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]): st.markdown(m["content"])

    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦")
    if user_msg:
        # ì‚¬ìš©ì ë§í’ì„  + ê¸°ë¡
        with st.chat_message("user"): st.markdown(user_msg)
        st.session_state["messages"].append({"role":"user","content":user_msg})
        if mem_on: add_dialog(session_id, "user", user_msg)

        # ì•ˆì „ ì‘ë‹µ ìƒì„±(ì˜ˆì™¸/ë¹ˆì‘ë‹µ ë°©ì–´)
        try:
            answer_text, logs = compose_answer(
                user_msg,
                [s.strip() for s in engines.split(",") if s.strip()],
                why_chain,
                session_id
            )
        except Exception as e:
            warn = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸({type(e).__name__}). Mockë¡œ í´ë°±í•©ë‹ˆë‹¤.\n"
            mock = "ìš”ì§€: " + " ".join((identity_text()+user_msg).split()[:80])
            answer_text = warn + mock
            logs = [{"i":0,"by":"Mock","text":warn}]

        if not (answer_text or "").strip():
            answer_text = "â€» ì—”ì§„ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. í‚¤/ì¿¼í„° í™•ì¸ ìš”ë§. ì„ì‹œ ìš”ì•½ í‘œì‹œ.\n" \
                          "ìš”ì§€: " + " ".join(user_msg.split()[:50])

        # ì¢Œì¸¡ ë§í’ì„ ì— ë°˜ë“œì‹œ ì¶œë ¥(í† ë§‰ ìŠ¤íŠ¸ë¦¼ ëŠë‚Œ)
        with st.chat_message("assistant"):
            ph = st.empty(); shown=""
            for chunk in re.findall(r".{1,70}", answer_text, flags=re.S):
                shown += chunk; ph.markdown(shown); time.sleep(0.01)
            ph.markdown(shown)

        # ìƒíƒœ/ë©”ëª¨ë¦¬ ê°±ì‹  & ì˜¤ë¥¸ìª½ ì‚¬ê³  ë¡œê·¸
        st.session_state["messages"].append({"role":"assistant","content":answer_text})
        if mem_on: add_dialog(session_id, "assistant", answer_text)
        st.session_state["last_logs"]=logs

with right:
    st.caption("ìš°ì¸¡: ì‚¬ê³  ë¡œê·¸(ë‹¨ê³„ë³„). ì‚¬ëŒì²˜ëŸ¼ 'ì™œ?'ë¥¼ ìºë©° ì§„í–‰.")
    logs = st.session_state.get("last_logs", [])
    if not logs: st.info("ëŒ€í™”í•˜ë©´ ì—¬ê¸° ì‚¬ê³  ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    else:
        for l in logs:
            with st.expander(f"{l['i']}. {l['by']} Â· ë‹¨ê³„ ì‚¬ê³ ", expanded=False):
                st.markdown(l["text"])

st.divider()
st.caption("í‚¤ê°€ ì—†ê±°ë‚˜ ì¿¼í„° ì´ˆê³¼ ì‹œ ìë™ í´ë°±(Mock) Â· build v3.3")