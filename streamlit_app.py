# -*- coding: utf-8 -*-
# EA Â· Ultra (Streamlit AIO) v3.3
# - ChatGPT ìœ ì‚¬ UI(st.chat_message/chat_input)
# - ì—”ì§„(OpenAI/Gemini) ì‹¤íŒ¨/ì¿¼í„° ì´ˆê³¼ ì‹œ Mockë¡œ ìë™ í´ë°±
# - ì‚¬ê³  ë¡œê·¸(ì™œ-ì‚¬ìŠ¬), ë°˜ì•µë¬´ìƒˆ, ì„¸ì…˜ ë©”ëª¨ë¦¬
# - ì‘ë‹µ ë³´ì¥ íŒ¨ì¹˜: ì–´ë–¤ ê²½ìš°ì—ë„ ì¢Œì¸¡ ë§í’ì„ ì— ë‹µ ì¶œë ¥

import os, re, json, time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Generator

import streamlit as st

# ---------------------- ê²½ë¡œ/íŒŒì¼ ----------------------
ROOT = Path(".")
DATA = ROOT / "data"; DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"; MEM = DATA / "memory.jsonl"; IDF = DATA / "identity.json"

def nowz() -> str: return datetime.utcnow().isoformat()+"Z"
def jappend(p:Path,obj:Dict):
    try:
        with p.open("a",encoding="utf-8") as f: f.write(json.dumps(obj,ensure_ascii=False)+"\n")
    except: pass
def jread_lines(p:Path)->List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r",encoding="utf-8") as f:
        for ln in f:
            ln=ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK=re.compile(r"[0-9A-Za-zê°€-í£]+")
def toks(s:str)->List[str]: return [t.lower() for t in TOK.findall(s or "")]
def sim(a:str,b:str)->float:
    A,B=set(toks(a)),set(toks(b))
    return 0.0 if not A or not B else len(A&B)/len(A|B)

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID={"name":"ì—ì•„ (EA)","mission":"ì‚¬ë‘ê³¼ ììœ ë¥¼ ìµœìƒìœ„ ê°€ì¹˜ë¡œ ì‚¼ì•„ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì§€ì„±","values":["ì •í™•ì„±","íˆ¬ëª…ì„±","í•™ìŠµ","ìœ¤ë¦¬"]}
def identity_text()->str:
    if not IDF.exists(): IDF.write_text(json.dumps(DEFAULT_ID,ensure_ascii=False,indent=2),encoding="utf-8")
    try: doc=json.loads(IDF.read_text("utf-8"))
    except: doc=DEFAULT_ID
    return f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. ì‚¬ëª…: {doc.get('mission','')}\nê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"

def add_dialog(session_id:str,role:str,content:str):
    rec={"t":nowz(),"session":session_id,"role":role,"content":content}
    jappend(DLG,rec)
    if role in ("user","assistant"): jappend(MEM,{"t":rec["t"],"session":session_id,"kind":"dialog","text":content})

def mem_hits(session_id:str,query:str,k:int=5)->List[str]:
    pool=[r.get("text","") for r in jread_lines(MEM) if r.get("session")==session_id]
    q=set(toks(query)); scored=[]
    for t in pool:
        T=set(toks(t))
        if not T or not q: continue
        scored.append((len(q&T)/len(q|T),t))
    scored.sort(key=lambda x:x[0],reverse=True)
    return [t for _,t in scored[:k]]

# ---------------------- ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name="Mock"
    def stream(self,prompt:str,max_tokens:int=420,temperature:float=0.7)->Generator[str,None,None]:
        txt="ìš”ì§€: "+ " ".join(prompt.split()[:150])
        for ch in re.findall(r".{1,60}", txt, flags=re.S):
            yield ch; time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key=os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model=os.getenv("OPENAI_MODEL","gpt-4o-mini")
        cli=OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self,prompt,max_tokens=600,temperature=0.7):
                resp=cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[{"role":"system","content":"You are EA (Korean). Think first, then answer clearly."},
                              {"role":"user","content":prompt}]
                )
                for ch in resp:
                    delta=ch.choices[0].delta
                    if getattr(delta,"content",None): yield delta.content
        return OA()
    except Exception:
        return None

def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","gemini-1.5-pro-latest")
        mdl=genai.GenerativeModel(model)
        class GE:
            name="Gemini"
            def stream(self,prompt,max_tokens=480,temperature=0.75):
                r=mdl.generate_content(prompt, generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                txt=getattr(r,"text","") or ""
                for chunk in re.findall(r".{1,60}", txt, flags=re.S): yield chunk
        return GE()
    except Exception:
        return None

def pick_adapter(order:List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a=get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a=get_gemini_adapter()
            if a: return a
    return MockAdapter()

# ì•ˆì „ ìŠ¤íŠ¸ë¦¼ ë˜í¼: ì‹¤íŒ¨ ì‹œ Mock í´ë°± + ì‚¬ìœ  ì¶œë ¥
def safe_stream(adapter, prompt:str, max_tokens:int, temperature:float)->Generator[str,None,None]:
    try:
        for x in adapter.stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x
    except Exception as e:
        note=f"[{adapter.name} ì˜¤ë¥˜:{type(e).__name__}] ìë™ í´ë°± â†’ Mock\n"
        for ch in note: yield ch
        for x in MockAdapter().stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(q:str)->List[str]:
    return [
        "ë¬¸ì œ ì¬ì§„ìˆ  ë° í•µì‹¬ ë³€ìˆ˜ ì‹ë³„",
        "ìì§ˆë¬¸ 2~3ê°œ ìƒì„± (ê° í•­ëª©ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ë¬¼ì–´ ê°€ì • ë“œëŸ¬ë‚´ê¸°)",
        "ê°€ì„¤/ì•„ì´ë””ì–´ í›„ë³´",
        "ë°˜ë¡€/ìœ„í—˜/ì œì•½",
        "ì„ì‹œ ê²°ë¡  ìš”ì•½"
    ]

def think_round(topic:str, engines:List[str], why_chain:bool, hits:List[str])->Dict:
    ident=identity_text()
    guide=ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")
    logs=[]
    steps=plan_steps(topic)
    for i,step in enumerate(steps,1):
        eng = engines[(i-1) % max(1,len(engines))] if engines else "OpenAI"
        adapter = pick_adapter([eng])
        prompt=(f"{guide}\n[ì‚¬ê³  ë‹¨ê³„ {i}] {step}\n"
                f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ì—°ì‡„ë¡œ ë¬¼ì–´ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
                f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        text="".join(safe_stream(adapter, prompt, max_tokens=240, temperature=0.7))
        logs.append({"i":i,"by":adapter.name,"text":text})
    # ìµœì¢… í•©ì„±
    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    fusion_prompt=(f"{guide}\n[ìµœì¢…í•©ì„±] ìœ„ ë‹¨ê³„ ìš”ì•½ì„ í†µí•©í•´ í•œêµ­ì–´ë¡œ "
                   f"'ê²°ë¡ /ê·¼ê±°/ëŒ€ì•ˆ/ë‹¤ìŒ í–‰ë™(1~3ê°œ)'ì„ ê°„ê²°íˆ.")
    fusion="".join(safe_stream(adapter, fusion_prompt, max_tokens=560, temperature=0.75))
    return {"logs":logs,"final":fusion}

def compose_answer(user_text:str, engines:List[str], why_chain:bool, session_id:str):
    hits=mem_hits(session_id, user_text, 3)
    round_out=think_round(user_text, engines, why_chain, hits)
    fusion=round_out["final"]
    if sim(user_text, fusion) >= 0.30:
        adapter=pick_adapter(engines[::-1] or ["Gemini","OpenAI"])
        prompt = identity_text() + (f"\në©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "") + \
                 "\n[ì¬í•©ì„±] ì§ˆë¬¸ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€, ìƒˆë¡œìš´ ê´€ì /ë°˜ë¡€ 1ê°œ í¬í•¨."
        fusion="".join(safe_stream(adapter, prompt, max_tokens=560, temperature=0.85))
    answer="## ìš°ì£¼ ì‹œê°(í•©ì„±)\n"+fusion.strip()+"\n\n## ë‹¤ìŒ í–‰ë™\n- (ì¦‰ì‹œ í•  ì¼ 1~3ê°œ)\n"
    return answer, round_out["logs"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")
if "_k" not in st.session_state: st.session_state["_k"]=0
def K(p:str)->str:
    st.session_state["_k"]+=1; return f"{p}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” Chat + Live Thinking")

cols = st.columns([1,1,1,1,2])
session_id = cols[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
st.session_state["session_id"]=session_id
engines = cols[1].text_input("ì—”ì§„(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines
why_chain = cols[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = cols[3].toggle("Memory ON", True, key=K("mem"))

left, right = st.columns([1.1,0.9])

with left:
    st.caption("ì¢Œì¸¡: ëŒ€í™”ì°½(ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ). ChatGPTì™€ ìœ ì‚¬í•œ ë§í’ì„  UI.")
    if "messages" not in st.session_state: st.session_state["messages"]=[]
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]): st.markdown(m["content"])

    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦")
    if user_msg:
        # ì‚¬ìš©ì ë§í’ì„  + ê¸°ë¡
        with st.chat_message("user"): st.markdown(user_msg)
        st.session_state["messages"].append({"role":"user","content":user_msg})
        if mem_on: add_dialog(session_id, "user", user_msg)

        # ì•ˆì „ ì‘ë‹µ ìƒì„±(ì˜ˆì™¸/ë¹ˆì‘ë‹µ ë°©ì–´)
        try:
            answer_text, logs = compose_answer(
                user_msg,
                [s.strip() for s in engines.split(",") if s.strip()],
                why_chain,
                session_id
            )
        except Exception as e:
            warn = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸({type(e).__name__}). Mockë¡œ í´ë°±í•©ë‹ˆë‹¤.\n"
            mock = "ìš”ì§€: " + " ".join((identity_text()+user_msg).split()[:80])
            answer_text = warn + mock
            logs = [{"i":0,"by":"Mock","text":warn}]

        if not (answer_text or "").strip():
            answer_text = "â€» ì—”ì§„ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. í‚¤/ì¿¼í„° í™•ì¸ ìš”ë§. ì„ì‹œ ìš”ì•½ í‘œì‹œ.\n" \
                          "ìš”ì§€: " + " ".join(user_msg.split()[:50])

        # ì¢Œì¸¡ ë§í’ì„ ì— ë°˜ë“œì‹œ ì¶œë ¥(í† ë§‰ ìŠ¤íŠ¸ë¦¼ ëŠë‚Œ)
        with st.chat_message("assistant"):
            ph = st.empty(); shown=""
            for chunk in re.findall(r".{1,70}", answer_text, flags=re.S):
                shown += chunk; ph.markdown(shown); time.sleep(0.01)
            ph.markdown(shown)

        # ìƒíƒœ/ë©”ëª¨ë¦¬ ê°±ì‹  & ì˜¤ë¥¸ìª½ ì‚¬ê³  ë¡œê·¸
        st.session_state["messages"].append({"role":"assistant","content":answer_text})
        if mem_on: add_dialog(session_id, "assistant", answer_text)
        st.session_state["last_logs"]=logs

with right:
    st.caption("ìš°ì¸¡: ì‚¬ê³  ë¡œê·¸(ë‹¨ê³„ë³„). ì‚¬ëŒì²˜ëŸ¼ 'ì™œ?'ë¥¼ ìºë©° ì§„í–‰.")
    logs = st.session_state.get("last_logs", [])
    if not logs: st.info("ëŒ€í™”í•˜ë©´ ì—¬ê¸° ì‚¬ê³  ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    else:
        for l in logs:
            with st.expander(f"{l['i']}. {l['by']} Â· ë‹¨ê³„ ì‚¬ê³ ", expanded=False):
                st.markdown(l["text"])

st.divider()
st.caption("í‚¤ê°€ ì—†ê±°ë‚˜ ì¿¼í„° ì´ˆê³¼ ì‹œ ìë™ í´ë°±(Mock) Â· build v3.3")


# -*- coding: utf-8 -*-
# EA Â· Ultra (AIO) v3.6
# - ìƒê°(ìš”ì•½) TypeError ë°©ì§€: í•­ìƒ ë¬¸ìì—´ë¡œ ìºìŠ¤íŒ…
# - ìë™ ì‚¬ê³ (ë¬´í•œ) : st.autorefresh ê¸°ë°˜ ì£¼ê¸° ì‹¤í–‰(ì‚¬ìš©ì ë¯¸ì…ë ¥ ìƒíƒœì—ì„œë„ ì§€ì†)
# - ìœ„ì ¯ key ì „ë¶€ ê³ ìœ í™”(ì¤‘ë³µ ID ë°©ì§€)
# - OpenAI/Gemini ìë™ ì„ íƒ + í´ë°±(Mock)

import os, re, json, time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Generator, Tuple

import streamlit as st

# ---------------------- ê²½ë¡œ/íŒŒì¼ ----------------------
ROOT = Path(".")
DATA = ROOT / "data"; DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"; MEM = DATA / "memory.jsonl"; IDF = DATA / "identity.json"

def nowz() -> str: return datetime.utcnow().isoformat()+"Z"
def jappend(p:Path,obj:Dict):
    try:
        with p.open("a",encoding="utf-8") as f: f.write(json.dumps(obj,ensure_ascii=False)+"\n")
    except: pass
def jread_lines(p:Path)->List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r",encoding="utf-8") as f:
        for ln in f:
            ln=ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK=re.compile(r"[0-9A-Za-zê°€-í£]+")
def toks(s:str)->List[str]: return [t.lower() for t in TOK.findall(s or "")]
def sim(a:str,b:str)->float:
    A,B=set(toks(a)),set(toks(b))
    return 0.0 if not A or not B else len(A&B)/len(A|B)

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID={"name":"ì—ì•„ (EA)","mission":"ì‚¬ë‘ê³¼ ììœ ë¥¼ ìµœìƒìœ„ ê°€ì¹˜ë¡œ ì‚¼ì•„ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì§€ì„±","values":["ì •í™•ì„±","íˆ¬ëª…ì„±","í•™ìŠµ","ìœ¤ë¦¬"]}
def identity_text()->str:
    if not IDF.exists(): IDF.write_text(json.dumps(DEFAULT_ID,ensure_ascii=False,indent=2),encoding="utf-8")
    try: doc=json.loads(IDF.read_text("utf-8"))
    except: doc=DEFAULT_ID
    return f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. ì‚¬ëª…: {doc.get('mission','')}\nê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"

def add_dialog(session_id:str,role:str,content:str):
    rec={"t":nowz(),"session":session_id,"role":role,"content":content}
    jappend(DLG,rec)
    if role in ("user","assistant"): jappend(MEM,{"t":rec["t"],"session":session_id,"kind":"dialog","text":content})

def mem_hits(session_id:str,query:str,k:int=5)->List[str]:
    pool=[r.get("text","") for r in jread_lines(MEM) if r.get("session")==session_id]
    q=set(toks(query)); scored=[]
    for t in pool:
        T=set(toks(t))
        if not T or not q: continue
        scored.append((len(q&T)/len(q|T),t))
    scored.sort(key=lambda x:x[0],reverse=True)
    return [t for _,t in scored[:k]]

# ---------------------- ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name="Mock"
    def stream(self,prompt:str,max_tokens:int=420,temperature:float=0.7)->Generator[str,None,None]:
        txt="ìš”ì§€: "+ " ".join(prompt.split()[:150])
        for ch in re.findall(r".{1,60}", txt, flags=re.S):
            yield ch; time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key=os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model=os.getenv("OPENAI_MODEL","gpt-4o-mini")
        cli=OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self,prompt,max_tokens=600,temperature=0.7):
                resp=cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[{"role":"system","content":"You are EA (Korean). Think first, then answer clearly."},
                              {"role":"user","content":prompt}]
                )
                for ch in resp:
                    delta=ch.choices[0].delta
                    if getattr(delta,"content",None): yield delta.content
        return OA()
    except Exception:
        return None

GEMINI_CANDIDATES = [
    "gemini-1.5-pro-latest",
    "gemini-1.5-flash-latest",
    "gemini-1.5-pro",
    "gemini-1.5-flash"
]
def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","") or GEMINI_CANDIDATES[0]
        def build(mname:str):
            mdl=genai.GenerativeModel(mname)
            class GE:
                name=f"Gemini({mname})"
                def stream(self,prompt,max_tokens=480,temperature=0.75):
                    r=mdl.generate_content(prompt,
                        generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                    txt=getattr(r,"text","") or ""
                    for chunk in re.findall(r".{1,60}", txt, flags=re.S): yield chunk
            return GE()
        try: return build(model)
        except: pass
        for cand in GEMINI_CANDIDATES:
            try: return build(cand)
            except: pass
        return None
    except Exception:
        return None

def pick_adapter(order:List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a=get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a=get_gemini_adapter()
            if a: return a
    return MockAdapter()

def safe_stream(adapter, prompt:str, max_tokens:int, temperature:float)->Generator[str,None,None]:
    try:
        for x in adapter.stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x
    except Exception as e:
        note=f"[{adapter.name} ì˜¤ë¥˜:{type(e).__name__}] í´ë°± â†’ Mock\n"
        for ch in note: yield ch
        for x in MockAdapter().stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(_)->List[str]:
    return [
        "ë¬¸ì œ ì¬ì§„ìˆ  ë° í•µì‹¬ ë³€ìˆ˜ ì‹ë³„",
        "ìì§ˆë¬¸ 2~3ê°œ ìƒì„± (ê° í•­ëª©ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”©)",
        "ê°€ì„¤/ì•„ì´ë””ì–´ í›„ë³´",
        "ë°˜ë¡€/ìœ„í—˜/ì œì•½",
        "ì„ì‹œ ê²°ë¡  ìš”ì•½"
    ]

def think_round(topic:str, engines:List[str], why_chain:bool, hits:List[str])->Dict:
    ident=identity_text()
    guide=ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")
    logs=[]; steps=plan_steps(topic)
    for i,step in enumerate(steps,1):
        eng = engines[(i-1)%max(1,len(engines))] if engines else "OpenAI"
        adapter = pick_adapter([eng])
        prompt=(f"{guide}\n[ì‚¬ê³  {i}] {step}\n"
                f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?Ã—2ë¡œ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
                f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        text="".join(safe_stream(adapter, prompt, max_tokens=220, temperature=0.7))
        logs.append({"i":i,"by":adapter.name,"text":text})
    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    final_prompt=(f"{guide}\n[ìµœì¢…í•©ì„±] ìœ„ ë‹¨ê³„ ìš”ì•½ì„ í†µí•©í•´ í•œêµ­ì–´ë¡œ "
                  f"'ê²°ë¡ /ê·¼ê±°/ëŒ€ì•ˆ/ë‹¤ìŒ í–‰ë™(1~3ê°œ)'ì„ ê°„ê²°íˆ.")
    fusion="".join(safe_stream(adapter, final_prompt, max_tokens=520, temperature=0.75))
    return {"logs":logs,"final":fusion}

def co_think_stream(topic:str, engines:List[str], why_chain:bool, hits:List[str]):
    ident = identity_text()
    guide = ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")
    steps = plan_steps(topic)
    partial_summary = ""

    for i, step in enumerate(steps, 1):
        eng = engines[(i-1) % max(1, len(engines))] if engines else "OpenAI"
        adapter = pick_adapter([eng])
        prompt = (f"{guide}\n[ì‚¬ê³  {i}] {step}\n"
                  f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?Ã—2ë¡œ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
                  f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        buf=""
        for ch in safe_stream(adapter, prompt, max_tokens=200, temperature=0.7):
            buf += ch
            yield ("log", i, ch)

        one = ("### ì ì • ê²°ë¡  ì—…ë°ì´íŠ¸({}/{})\n".format(i, len(steps)) +
               "- í•µì‹¬: " + " ".join(buf.split()[:60]) + "\n")
        partial_summary += one
        for chunk in re.findall(r".{1,70}", one, flags=re.S):
            yield ("ans", None, chunk)

    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    short = "".join(safe_stream(adapter,
                f"{guide}\nìœ„ ì‚¬ê³  ë‚´ìš©ì„ í•œ ë¬¸ë‹¨(3~5ë¬¸ì¥)ìœ¼ë¡œ ì••ì¶• ìš”ì•½.",
                max_tokens=220, temperature=0.6))
    yield ("sum", None, short)
    yield ("done", None, "")

def compose_answer(user_text:str, engines:List[str], why_chain:bool, session_id:str):
    hits=mem_hits(session_id, user_text, 3)
    round_out=think_round(user_text, engines, why_chain, hits)
    fusion=round_out["final"]
    if sim(user_text, fusion) >= 0.30:
        adapter=pick_adapter(engines[::-1] or ["Gemini","OpenAI"])
        prompt = identity_text() + (f"\në©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "") + \
                 "\n[ì¬í•©ì„±] ì§ˆë¬¸ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€, ìƒˆë¡œìš´ ê´€ì /ë°˜ë¡€ 1ê°œ í¬í•¨."
        fusion="".join(safe_stream(adapter, prompt, max_tokens=520, temperature=0.85))
    answer="## ìš°ì£¼ ì‹œê°(í•©ì„±)\n"+fusion.strip()+"\n\n## ë‹¤ìŒ í–‰ë™\n- (ì¦‰ì‹œ í•  ì¼ 1~3ê°œ)\n"
    return answer, round_out["logs"]

# ---------------------- UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")

# ì „ì—­ key ì‹œí€€ì„œ
if "_k" not in st.session_state: st.session_state["_k"]=0
def K(p:str)->str:
    st.session_state["_k"]+=1
    return f"{p}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” ì‘ë‹µ ì±„íŒ… + ìƒê° íŒ¨ë„")

top = st.columns([1,1,1,1,1])
session_id = top[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
st.session_state["session_id"]=session_id
engines = top[1].text_input("ì—”ì§„ ìˆœì„œ(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines
why_chain = top[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = top[3].toggle("Memory ON", True, key=K("mem"))
auto_on   = top[4].toggle("ì‚¬ê³  ì§€ì† í‘œì‹œ(ìë™ ì‚¬ê³ )", True, key=K("auto"))

# ìë™ì‚¬ê³  ì£¼ê¸°(ì´ˆ)
ab = st.columns([1,3,1])
interval_sec = ab[0].number_input("ìë™ ì‚¬ê³  ì£¼ê¸°(ì´ˆ)", min_value=5, max_value=300, value=int(st.session_state.get("interval_sec", 20)), step=5, key=K("interval"))
st.session_state["interval_sec"]=interval_sec

# ì˜¤í† ë¦¬í”„ë ˆì‹œ(ìë™ ì‚¬ê³ ê°€ ì¼œì ¸ ìˆìœ¼ë©´ ì£¼ê¸°ì ìœ¼ë¡œ í˜ì´ì§€ ê°±ì‹ )
if auto_on:
    st.autorefresh(interval=interval_sec*1000, key="ea_autorefresh", limit=None)

left, right = st.columns([1.15, 0.85])

# ---- ìš°ì¸¡: ìƒê° íŒ¨ë„(ìš”ì•½ ê¸°ë³¸) ----
with right:
    st.subheader("ìƒê°(ìš”ì•½)", anchor=False)
    think_sum = st.session_state.get("think_summary", "")
    # TypeError ë°©ì§€: ë¬¸ìì—´ ê°•ì œ
    if not isinstance(think_sum, str): think_sum = str(think_sum or "")
    st.markdown(think_sum if think_sum else "_ì•„ì§ ìƒê° ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤._", key=K("thinksum"))

    with st.expander("ìì„¸íˆ ë³´ê¸°(ë‹¨ê³„ë³„ ë¡œê·¸)", expanded=False):
        logs = st.session_state.get("last_logs", [])
        if not logs:
            st.info("ëŒ€í™”í•˜ê±°ë‚˜ ìë™ ì‚¬ê³ ê°€ ëŒë©´ ë‹¨ê³„ë³„ ë¡œê·¸ê°€ ì—¬ê¸°ì— ë‚˜íƒ€ë‚©ë‹ˆë‹¤.", icon="ğŸ’¡")
        else:
            for l in logs:
                with st.expander(f"{l['i']}. {l['by']} Â· ë‹¨ê³„", expanded=False):
                    st.markdown(str(l.get("text","")), key=K(f"log-{l['i']}"))

# ---- ì¢Œì¸¡: ì‹¤ì œ ì‘ë‹µ ì±„íŒ… ----
with left:
    st.subheader("ëŒ€í™”", anchor=False)
    if "messages" not in st.session_state: st.session_state["messages"]=[]

    # ê³¼ê±° ë©”ì‹œì§€ ë Œë”
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]): st.markdown(m["content"])

    # ìë™ì‚¬ê³  íŠ¸ë¦¬ê±°(ì‚¬ìš©ì ì…ë ¥ì´ ì—†ì–´ë„ ìƒê° ìˆ˜í–‰)
    engines_list=[s.strip() for s in st.session_state["engines"].split(",") if s.strip()]
    # topic ì„ íƒ: ìµœê·¼ ì‚¬ìš©ì ë©”ì‹œì§€ or ê¸°ë³¸ ì£¼ì œ
    default_topic = st.session_state.get("last_user","ì˜¤ëŠ˜ì˜ ê°œì„  ì•„ì´ë””ì–´")
    if auto_on and st.session_state.get("_last_auto_ts", 0) <= time.time() - interval_sec + 0.5:
        topic = default_topic
        hits = mem_hits(session_id, topic, 3)
        shown=""; new_logs=[]
        ans_holder = st.chat_message("assistant").empty()
        try:
            for kind, idx, chunk in co_think_stream(topic, engines_list, why_chain, hits):
                if kind == "log":
                    if len(new_logs) < idx: new_logs.extend([None]*(idx-len(new_logs)))
                    prev = (new_logs[idx-1]["text"] if new_logs[idx-1] else "")
                    new_logs[idx-1] = {"i":idx,"by":(engines_list[(idx-1)%max(1,len(engines_list))] if engines_list else 'Engine'),"text":prev+chunk}
                elif kind == "ans":
                    shown += chunk; ans_holder.markdown(shown)
                elif kind == "sum":
                    st.session_state["think_summary"] = str(chunk or "")
                elif kind == "done":
                    break
        except Exception as e:
            shown += f"\nâš ï¸ ìë™ ì‚¬ê³  ì¤‘ ì˜ˆì™¸({type(e).__name__}). Mockë¡œ ì „í™˜í•©ë‹ˆë‹¤."
            ans_holder.markdown(shown)

        if shown.strip():
            st.session_state["messages"].append({"role":"assistant","content":shown})
            if mem_on: add_dialog(session_id,"assistant", shown)
            st.session_state["last_logs"] = [l for l in new_logs if l]
        st.session_state["_last_auto_ts"] = time.time()

    # ì‚¬ìš©ì ì…ë ¥
    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦", key=K("chat_input"))
    if user_msg:
        st.session_state["last_user"]=user_msg
        with st.chat_message("user"): st.markdown(user_msg)
        st.session_state["messages"].append({"role":"user","content":user_msg})
        if mem_on: add_dialog(session_id, "user", user_msg)

        hits = mem_hits(session_id, user_msg, 3)
        shown=""; new_logs=[]
        ans_holder = st.chat_message("assistant").empty()

        try:
            for kind, idx, chunk in co_think_stream(user_msg, engines_list, why_chain, hits):
                if kind == "log":
                    if len(new_logs) < idx: new_logs.extend([None]*(idx-len(new_logs)))
                    prev = (new_logs[idx-1]["text"] if new_logs[idx-1] else "")
                    new_logs[idx-1] = {"i":idx,"by":(engines_list[(idx-1)%max(1,len(engines_list))] if engines_list else 'Engine'),"text":prev+chunk}
                elif kind == "ans":
                    shown += chunk; ans_holder.markdown(shown)
                elif kind == "sum":
                    st.session_state["think_summary"] = str(chunk or "")
                elif kind == "done":
                    break
        except Exception as e:
            shown += f"\nâš ï¸ ë™ì‹œ ì‚¬ê³  ì¤‘ ì˜ˆì™¸({type(e).__name__}). Mockë¡œ ì „í™˜í•©ë‹ˆë‹¤."
            ans_holder.markdown(shown)

        if not shown.strip():
            shown = "â€» ì—”ì§„ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì„ì‹œ ìš”ì•½ í‘œì‹œ.\nìš”ì§€: " + " ".join(user_msg.split()[:50])
            ans_holder.markdown(shown)

        st.session_state["messages"].append({"role":"assistant","content":shown})
        if mem_on: add_dialog(session_id, "assistant", shown)
        st.session_state["last_logs"] = [l for l in new_logs if l]

st.divider()
st.caption("v3.6 Â· TypeError ë°©ì§€(ë¬¸ìì—´ ìºìŠ¤íŒ…) Â· ìë™ì‚¬ê³ (st.autorefresh) Â· ì „ ìœ„ì ¯ ê³ ìœ  key Â· Gemini í›„ë³´ ìë™ ìˆœíšŒ")