# -*- coding: utf-8 -*-
# EA Â· Ultra (Streamlit AIO) v3.3
# - ChatGPT ìœ ì‚¬ UI(st.chat_message/chat_input)
# - ì—”ì§„(OpenAI/Gemini) ì‹¤íŒ¨/ì¿¼í„° ì´ˆê³¼ ì‹œ Mockë¡œ ìë™ í´ë°±
# - ì‚¬ê³  ë¡œê·¸(ì™œ-ì‚¬ìŠ¬), ë°˜ì•µë¬´ìƒˆ, ì„¸ì…˜ ë©”ëª¨ë¦¬
# - ì‘ë‹µ ë³´ì¥ íŒ¨ì¹˜: ì–´ë–¤ ê²½ìš°ì—ë„ ì¢Œì¸¡ ë§í’ì„ ì— ë‹µ ì¶œë ¥

import os, re, json, time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Generator

import streamlit as st

# ---------------------- ê²½ë¡œ/íŒŒì¼ ----------------------
ROOT = Path(".")
DATA = ROOT / "data"; DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"; MEM = DATA / "memory.jsonl"; IDF = DATA / "identity.json"

def nowz() -> str: return datetime.utcnow().isoformat()+"Z"
def jappend(p:Path,obj:Dict):
    try:
        with p.open("a",encoding="utf-8") as f: f.write(json.dumps(obj,ensure_ascii=False)+"\n")
    except: pass
def jread_lines(p:Path)->List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r",encoding="utf-8") as f:
        for ln in f:
            ln=ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK=re.compile(r"[0-9A-Za-zê°€-í£]+")
def toks(s:str)->List[str]: return [t.lower() for t in TOK.findall(s or "")]
def sim(a:str,b:str)->float:
    A,B=set(toks(a)),set(toks(b))
    return 0.0 if not A or not B else len(A&B)/len(A|B)

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID={"name":"ì—ì•„ (EA)","mission":"ì‚¬ë‘ê³¼ ììœ ë¥¼ ìµœìƒìœ„ ê°€ì¹˜ë¡œ ì‚¼ì•„ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì§€ì„±","values":["ì •í™•ì„±","íˆ¬ëª…ì„±","í•™ìŠµ","ìœ¤ë¦¬"]}
def identity_text()->str:
    if not IDF.exists(): IDF.write_text(json.dumps(DEFAULT_ID,ensure_ascii=False,indent=2),encoding="utf-8")
    try: doc=json.loads(IDF.read_text("utf-8"))
    except: doc=DEFAULT_ID
    return f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. ì‚¬ëª…: {doc.get('mission','')}\nê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"

def add_dialog(session_id:str,role:str,content:str):
    rec={"t":nowz(),"session":session_id,"role":role,"content":content}
    jappend(DLG,rec)
    if role in ("user","assistant"): jappend(MEM,{"t":rec["t"],"session":session_id,"kind":"dialog","text":content})

def mem_hits(session_id:str,query:str,k:int=5)->List[str]:
    pool=[r.get("text","") for r in jread_lines(MEM) if r.get("session")==session_id]
    q=set(toks(query)); scored=[]
    for t in pool:
        T=set(toks(t))
        if not T or not q: continue
        scored.append((len(q&T)/len(q|T),t))
    scored.sort(key=lambda x:x[0],reverse=True)
    return [t for _,t in scored[:k]]

# ---------------------- ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name="Mock"
    def stream(self,prompt:str,max_tokens:int=420,temperature:float=0.7)->Generator[str,None,None]:
        txt="ìš”ì§€: "+ " ".join(prompt.split()[:150])
        for ch in re.findall(r".{1,60}", txt, flags=re.S):
            yield ch; time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key=os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model=os.getenv("OPENAI_MODEL","gpt-4o-mini")
        cli=OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self,prompt,max_tokens=600,temperature=0.7):
                resp=cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[{"role":"system","content":"You are EA (Korean). Think first, then answer clearly."},
                              {"role":"user","content":prompt}]
                )
                for ch in resp:
                    delta=ch.choices[0].delta
                    if getattr(delta,"content",None): yield delta.content
        return OA()
    except Exception:
        return None

def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","gemini-1.5-pro-latest")
        mdl=genai.GenerativeModel(model)
        class GE:
            name="Gemini"
            def stream(self,prompt,max_tokens=480,temperature=0.75):
                r=mdl.generate_content(prompt, generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                txt=getattr(r,"text","") or ""
                for chunk in re.findall(r".{1,60}", txt, flags=re.S): yield chunk
        return GE()
    except Exception:
        return None

def pick_adapter(order:List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a=get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a=get_gemini_adapter()
            if a: return a
    return MockAdapter()

# ì•ˆì „ ìŠ¤íŠ¸ë¦¼ ë˜í¼: ì‹¤íŒ¨ ì‹œ Mock í´ë°± + ì‚¬ìœ  ì¶œë ¥
def safe_stream(adapter, prompt:str, max_tokens:int, temperature:float)->Generator[str,None,None]:
    try:
        for x in adapter.stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x
    except Exception as e:
        note=f"[{adapter.name} ì˜¤ë¥˜:{type(e).__name__}] ìë™ í´ë°± â†’ Mock\n"
        for ch in note: yield ch
        for x in MockAdapter().stream(prompt, max_tokens=max_tokens, temperature=temperature):
            yield x

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(q:str)->List[str]:
    return [
        "ë¬¸ì œ ì¬ì§„ìˆ  ë° í•µì‹¬ ë³€ìˆ˜ ì‹ë³„",
        "ìì§ˆë¬¸ 2~3ê°œ ìƒì„± (ê° í•­ëª©ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ë¬¼ì–´ ê°€ì • ë“œëŸ¬ë‚´ê¸°)",
        "ê°€ì„¤/ì•„ì´ë””ì–´ í›„ë³´",
        "ë°˜ë¡€/ìœ„í—˜/ì œì•½",
        "ì„ì‹œ ê²°ë¡  ìš”ì•½"
    ]

def think_round(topic:str, engines:List[str], why_chain:bool, hits:List[str])->Dict:
    ident=identity_text()
    guide=ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")
    logs=[]
    steps=plan_steps(topic)
    for i,step in enumerate(steps,1):
        eng = engines[(i-1) % max(1,len(engines))] if engines else "OpenAI"
        adapter = pick_adapter([eng])
        prompt=(f"{guide}\n[ì‚¬ê³  ë‹¨ê³„ {i}] {step}\n"
                f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ì—°ì‡„ë¡œ ë¬¼ì–´ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
                f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        text="".join(safe_stream(adapter, prompt, max_tokens=240, temperature=0.7))
        logs.append({"i":i,"by":adapter.name,"text":text})
    # ìµœì¢… í•©ì„±
    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    fusion_prompt=(f"{guide}\n[ìµœì¢…í•©ì„±] ìœ„ ë‹¨ê³„ ìš”ì•½ì„ í†µí•©í•´ í•œêµ­ì–´ë¡œ "
                   f"'ê²°ë¡ /ê·¼ê±°/ëŒ€ì•ˆ/ë‹¤ìŒ í–‰ë™(1~3ê°œ)'ì„ ê°„ê²°íˆ.")
    fusion="".join(safe_stream(adapter, fusion_prompt, max_tokens=560, temperature=0.75))
    return {"logs":logs,"final":fusion}

def compose_answer(user_text:str, engines:List[str], why_chain:bool, session_id:str):
    hits=mem_hits(session_id, user_text, 3)
    round_out=think_round(user_text, engines, why_chain, hits)
    fusion=round_out["final"]
    if sim(user_text, fusion) >= 0.30:
        adapter=pick_adapter(engines[::-1] or ["Gemini","OpenAI"])
        prompt = identity_text() + (f"\në©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "") + \
                 "\n[ì¬í•©ì„±] ì§ˆë¬¸ ë¬¸êµ¬ ë°˜ë³µ ê¸ˆì§€, ìƒˆë¡œìš´ ê´€ì /ë°˜ë¡€ 1ê°œ í¬í•¨."
        fusion="".join(safe_stream(adapter, prompt, max_tokens=560, temperature=0.85))
    answer="## ìš°ì£¼ ì‹œê°(í•©ì„±)\n"+fusion.strip()+"\n\n## ë‹¤ìŒ í–‰ë™\n- (ì¦‰ì‹œ í•  ì¼ 1~3ê°œ)\n"
    return answer, round_out["logs"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")
if "_k" not in st.session_state: st.session_state["_k"]=0
def K(p:str)->str:
    st.session_state["_k"]+=1; return f"{p}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” Chat + Live Thinking")

cols = st.columns([1,1,1,1,2])
session_id = cols[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
st.session_state["session_id"]=session_id
engines = cols[1].text_input("ì—”ì§„(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines
why_chain = cols[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = cols[3].toggle("Memory ON", True, key=K("mem"))

left, right = st.columns([1.1,0.9])

with left:
    st.caption("ì¢Œì¸¡: ëŒ€í™”ì°½(ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ). ChatGPTì™€ ìœ ì‚¬í•œ ë§í’ì„  UI.")
    if "messages" not in st.session_state: st.session_state["messages"]=[]
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]): st.markdown(m["content"])

    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦")
    if user_msg:
        # ì‚¬ìš©ì ë§í’ì„  + ê¸°ë¡
        with st.chat_message("user"): st.markdown(user_msg)
        st.session_state["messages"].append({"role":"user","content":user_msg})
        if mem_on: add_dialog(session_id, "user", user_msg)

        # ì•ˆì „ ì‘ë‹µ ìƒì„±(ì˜ˆì™¸/ë¹ˆì‘ë‹µ ë°©ì–´)
        try:
            answer_text, logs = compose_answer(
                user_msg,
                [s.strip() for s in engines.split(",") if s.strip()],
                why_chain,
                session_id
            )
        except Exception as e:
            warn = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸({type(e).__name__}). Mockë¡œ í´ë°±í•©ë‹ˆë‹¤.\n"
            mock = "ìš”ì§€: " + " ".join((identity_text()+user_msg).split()[:80])
            answer_text = warn + mock
            logs = [{"i":0,"by":"Mock","text":warn}]

        if not (answer_text or "").strip():
            answer_text = "â€» ì—”ì§„ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. í‚¤/ì¿¼í„° í™•ì¸ ìš”ë§. ì„ì‹œ ìš”ì•½ í‘œì‹œ.\n" \
                          "ìš”ì§€: " + " ".join(user_msg.split()[:50])

        # ì¢Œì¸¡ ë§í’ì„ ì— ë°˜ë“œì‹œ ì¶œë ¥(í† ë§‰ ìŠ¤íŠ¸ë¦¼ ëŠë‚Œ)
        with st.chat_message("assistant"):
            ph = st.empty(); shown=""
            for chunk in re.findall(r".{1,70}", answer_text, flags=re.S):
                shown += chunk; ph.markdown(shown); time.sleep(0.01)
            ph.markdown(shown)

        # ìƒíƒœ/ë©”ëª¨ë¦¬ ê°±ì‹  & ì˜¤ë¥¸ìª½ ì‚¬ê³  ë¡œê·¸
        st.session_state["messages"].append({"role":"assistant","content":answer_text})
        if mem_on: add_dialog(session_id, "assistant", answer_text)
        st.session_state["last_logs"]=logs

with right:
    st.caption("ìš°ì¸¡: ì‚¬ê³  ë¡œê·¸(ë‹¨ê³„ë³„). ì‚¬ëŒì²˜ëŸ¼ 'ì™œ?'ë¥¼ ìºë©° ì§„í–‰.")
    logs = st.session_state.get("last_logs", [])
    if not logs: st.info("ëŒ€í™”í•˜ë©´ ì—¬ê¸° ì‚¬ê³  ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    else:
        for l in logs:
            with st.expander(f"{l['i']}. {l['by']} Â· ë‹¨ê³„ ì‚¬ê³ ", expanded=False):
                st.markdown(l["text"])

st.divider()
st.caption("í‚¤ê°€ ì—†ê±°ë‚˜ ì¿¼í„° ì´ˆê³¼ ì‹œ ìë™ í´ë°±(Mock) Â· build v3.3")


# -*- coding: utf-8 -*-
# EA Â· Ultra (AIO) v3.8
# - st.markdown key ì œê±°(êµ¬ë²„ì „ í˜¸í™˜) + ì•ˆì „ ìºìŠ¤íŒ…
# - auto-refresh í˜¸í™˜ ìœ ì§€, ë™ì‹œ ì‚¬ê³  ìŠ¤íŠ¸ë¦¼/ìš”ì•½/ë©”ëª¨ë¦¬/ì—”ì§„ í´ë°± ë™ì¼

import os, re, json, time
from pathlib import Path
from datetime import datetime
from typing import List, Dict

import streamlit as st

# ---------------------- íŒŒì¼/ìœ í‹¸ ----------------------
ROOT = Path("."); DATA = ROOT / "data"; DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"; MEM = DATA / "memory.jsonl"; IDF = DATA / "identity.json"

def nowz(): return datetime.utcnow().isoformat()+"Z"
def jappend(p:Path,obj:Dict):
    try:
        with p.open("a",encoding="utf-8") as f: f.write(json.dumps(obj,ensure_ascii=False)+"\n")
    except: pass
def jread_lines(p:Path)->List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r",encoding="utf-8") as f:
        for ln in f:
            ln=ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK=re.compile(r"[0-9A-Za-zê°€-í£]+")
def toks(s:str)->List[str]: return [t.lower() for t in TOK.findall(s or "")]
def sim(a:str,b:str)->float:
    A,B=set(toks(a)),set(toks(b))
    return 0.0 if not A or not B else len(A&B)/len(A|B)

# ì•ˆì „ ë§ˆí¬ë‹¤ìš´(êµ¬ë²„ì „ í˜¸í™˜: key ì‚¬ìš© ì•ˆ í•¨)
def md(x:str):
    st.markdown(str(x or ""))

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID={"name":"ì—ì•„ (EA)","mission":"ì‚¬ë‘Â·ììœ ë¥¼ í•µì‹¬ìœ¼ë¡œ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥","values":["ì •í™•ì„±","íˆ¬ëª…ì„±","í•™ìŠµ","ìœ¤ë¦¬"]}
def identity_text()->str:
    if not IDF.exists(): IDF.write_text(json.dumps(DEFAULT_ID,ensure_ascii=False,indent=2),encoding="utf-8")
    try: doc=json.loads(IDF.read_text("utf-8"))
    except: doc=DEFAULT_ID
    return f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. ì‚¬ëª…: {doc.get('mission','')}\nê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"

def add_dialog(sid:str,role:str,content:str):
    rec={"t":nowz(),"session":sid,"role":role,"content":content}
    jappend(DLG,rec)
    if role in ("user","assistant"): jappend(MEM,{"t":rec["t"],"session":sid,"kind":"dialog","text":content})

def mem_hits(sid:str,q:str,k:int=5)->List[str]:
    pool=[r.get("text","") for r in jread_lines(MEM) if r.get("session")==sid]
    Q=set(toks(q)); scored=[]
    for t in pool:
        T=set(toks(t))
        if not T or not Q: continue
        scored.append((len(Q&T)/len(Q|T),t))
    scored.sort(key=lambda x:x[0],reverse=True)
    return [t for _,t in scored[:k]]

# ---------------------- ëª¨ë¸ ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name="Mock"
    def stream(self,prompt:str,max_tokens:int=420,temperature:float=0.7):
        txt="ìš”ì§€: "+ " ".join(prompt.split()[:150])
        for ch in re.findall(r".{1,60}",txt,flags=re.S):
            yield ch; time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key=os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model=os.getenv("OPENAI_MODEL","gpt-4o-mini")
        cli=OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self,prompt,max_tokens=600,temperature=0.7):
                resp=cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[{"role":"system","content":"You are EA (Korean). Think first, then answer clearly."},
                              {"role":"user","content":prompt}]
                )
                for ev in resp:
                    d=ev.choices[0].delta
                    if getattr(d,"content",None): yield d.content
        return OA()
    except Exception:
        return None

GEMINI_CANDIDATES=["gemini-1.5-pro-latest","gemini-1.5-flash-latest","gemini-1.5-pro","gemini-1.5-flash"]
def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","") or GEMINI_CANDIDATES[0]
        def build(mname):
            mdl=genai.GenerativeModel(mname)
            class GE:
                name=f"Gemini({mname})"
                def stream(self,prompt,max_tokens=480,temperature=0.75):
                    r=mdl.generate_content(prompt,generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                    txt=getattr(r,"text","") or ""
                    for ch in re.findall(r".{1,60}",txt,flags=re.S): yield ch
            return GE()
        try: return build(model)
        except: pass
        for c in GEMINI_CANDIDATES:
            try: return build(c)
            except: pass
        return None
    except Exception:
        return None

def pick_adapter(order:List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a=get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a=get_gemini_adapter()
            if a: return a
    return MockAdapter()

def safe_stream(adapter, prompt, max_tokens, temperature):
    try:
        for x in adapter.stream(prompt,max_tokens=max_tokens,temperature=temperature):
            yield x
    except Exception as e:
        note=f"[{adapter.name} ì˜¤ë¥˜:{type(e).__name__}] í´ë°± â†’ Mock\n"
        for ch in note: yield ch
        for x in MockAdapter().stream(prompt,max_tokens=max_tokens,temperature=temperature):
            yield x

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(_): 
    return ["ë¬¸ì œ ì¬ì§„ìˆ /í•µì‹¬ ë³€ìˆ˜","ìì§ˆë¬¸ ìƒì„±(ì™œÃ—2)","ê°€ì„¤/ì•„ì´ë””ì–´","ë°˜ë¡€/ìœ„í—˜","ì„ì‹œ ê²°ë¡ "]

def co_think_stream(topic, engines, why_chain, hits):
    ident=identity_text()
    guide=ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- "+"\n- ".join(hits)+"\n" if hits else "")
    steps=plan_steps(topic)
    partial=""
    for i,step in enumerate(steps,1):
        eng = engines[(i-1)%max(1,len(engines))] if engines else "OpenAI"
        adapter=pick_adapter([eng])
        prompt=(f"{guide}\n[ì‚¬ê³  {i}] {step}\n"
                f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?Ã—2.' if why_chain else ''}\n"
                f"ì£¼ì œ: {topic}\n- ìš”ì•½:")
        buf=""
        for ch in safe_stream(adapter,prompt,200,0.7):
            buf+=ch; yield ("log",i,ch)
        one=f"### ì ì • ê²°ë¡  ì—…ë°ì´íŠ¸({i}/{len(steps)})\n- í•µì‹¬: "+" ".join(buf.split()[:60])+"\n"
        partial+=one
        for ch in re.findall(r".{1,70}",one,flags=re.S):
            yield ("ans",None,ch)
    adapter=pick_adapter(engines or ["OpenAI","Gemini"])
    short="".join(safe_stream(adapter,f"{guide}\nìœ„ ì‚¬ê³ ë¥¼ 3~5ë¬¸ì¥ìœ¼ë¡œ ì••ì¶• ìš”ì•½.",220,0.6))
    yield ("sum",None,short); yield ("done",None,"")

# ---------------------- UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")

if "_k" not in st.session_state: st.session_state["_k"]=0
def K(p:str)->str:
    st.session_state["_k"]+=1; return f"{p}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” ì‘ë‹µ ì±„íŒ… + ìƒê° íŒ¨ë„")

top = st.columns([1,1,1,1,1])
sid = top[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
st.session_state["session_id"]=sid
engines_txt = top[1].text_input("ì—”ì§„ ìˆœì„œ(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines_txt
why_chain = top[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = top[3].toggle("Memory ON", True, key=K("mem"))
auto_on   = top[4].toggle("ì‚¬ê³  ì§€ì† í‘œì‹œ(ìë™ ì‚¬ê³ )", True, key=K("auto"))

ab = st.columns([1,3,1])
interval_sec = ab[0].number_input("ìë™ ì‚¬ê³  ì£¼ê¸°(ì´ˆ)", min_value=5, max_value=300,
                                  value=int(st.session_state.get("interval_sec",20)),
                                  step=5, key=K("interval"))
st.session_state["interval_sec"]=interval_sec

# ====== í˜¸í™˜ ìë™ìƒˆë¡œê³ ì¹¨ ======
HAS_AUTOREFRESH = hasattr(st, "autorefresh")
HAS_RERUN = hasattr(st, "experimental_rerun")
if auto_on:
    if HAS_AUTOREFRESH:
        st.autorefresh(interval=interval_sec*1000, key="ea_auto", limit=None)
    elif HAS_RERUN:
        last = st.session_state.get("_tick", 0.0)
        if time.time() - last >= interval_sec - 0.2:
            st.session_state["_tick"] = time.time()
            st.experimental_rerun()
    else:
        st.info("ìë™ ìƒˆë¡œê³ ì¹¨ ë¯¸ì§€ì› í™˜ê²½ì…ë‹ˆë‹¤. ì…ë ¥ìœ¼ë¡œ ì‚¬ê³ ë¥¼ íŠ¸ë¦¬ê±°í•˜ì„¸ìš”.", icon="ğŸ”")

left, right = st.columns([1.15,0.85])

# ---- ìš°ì¸¡ ìƒê° íŒ¨ë„ ----
with right:
    st.subheader("ìƒê°(ìš”ì•½)")
    think_sum = st.session_state.get("think_summary","")
    md(think_sum if isinstance(think_sum,str) and think_sum.strip() else "_ì•„ì§ ìƒê° ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤._")
    with st.expander("ìì„¸íˆ ë³´ê¸°(ë‹¨ê³„ë³„ ë¡œê·¸)", expanded=False):
        logs = st.session_state.get("last_logs", [])
        if not logs: st.info("ëŒ€í™”/ìë™ ì‚¬ê³ ê°€ ëŒë©´ ë‹¨ê³„ë³„ ë¡œê·¸ê°€ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤.", icon="ğŸ’¡")
        else:
            for l in logs:
                with st.expander(f"{l.get('i','?')}. {l.get('by','Engine')} Â· ë‹¨ê³„", expanded=False):
                    md(l.get("text",""))

# ---- ì¢Œì¸¡ ëŒ€í™” ----
with left:
    st.subheader("ëŒ€í™”")
    if "messages" not in st.session_state: st.session_state["messages"]=[]

    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]): md(m["content"])

    engines = [s.strip() for s in st.session_state["engines"].split(",") if s.strip()]
    default_topic = st.session_state.get("last_user","ì˜¤ëŠ˜ì˜ ê°œì„  ì•„ì´ë””ì–´")

    # ìë™ ì‚¬ê³ 
    if auto_on and st.session_state.get("_last_auto_ts",0) <= time.time()-interval_sec+0.5:
        topic = default_topic
        hits = mem_hits(sid, topic, 3)
        shown=""; new_logs=[]; ans_holder = st.chat_message("assistant").empty()
        try:
            for kind, idx, chunk in co_think_stream(topic, engines, why_chain, hits):
                if kind=="log":
                    if len(new_logs) < idx: new_logs.extend([None]*(idx-len(new_logs)))
                    prev = (new_logs[idx-1]["text"] if new_logs[idx-1] else "")
                    new_logs[idx-1] = {"i":idx,"by":(engines[(idx-1)%max(1,len(engines))] if engines else 'Engine'),"text":prev+chunk}
                elif kind=="ans":
                    shown+=chunk; ans_holder.markdown(shown)
                elif kind=="sum":
                    st.session_state["think_summary"]=str(chunk or "")
                elif kind=="done":
                    break
        except Exception as e:
            shown += f"\nâš ï¸ ìë™ ì‚¬ê³  ì˜ˆì™¸({type(e).__name__}). Mock ì „í™˜."
            ans_holder.markdown(shown)
        if shown.strip():
            st.session_state["messages"].append({"role":"assistant","content":shown})
            if mem_on: add_dialog(sid,"assistant",shown)
            st.session_state["last_logs"]=[l for l in new_logs if l]
        st.session_state["_last_auto_ts"]=time.time()

    # ì‚¬ìš©ì ì…ë ¥
    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦", key=K("chat_input"))
    if user_msg:
        st.session_state["last_user"]=user_msg
        with st.chat_message("user"): md(user_msg)
        st.session_state["messages"].append({"role":"user","content":user_msg})
        if mem_on: add_dialog(sid,"user",user_msg)

        hits = mem_hits(sid, user_msg, 3)
        shown=""; new_logs=[]; ans_holder = st.chat_message("assistant").empty()
        try:
            for kind, idx, chunk in co_think_stream(user_msg, engines, why_chain, hits):
                if kind=="log":
                    if len(new_logs) < idx: new_logs.extend([None]*(idx-len(new_logs)))
                    prev = (new_logs[idx-1]["text"] if new_logs[idx-1] else "")
                    new_logs[idx-1] = {"i":idx,"by":(engines[(idx-1)%max(1,len(engines))] if engines else 'Engine'),"text":prev+chunk}
                elif kind=="ans":
                    shown+=chunk; ans_holder.markdown(shown)
                elif kind=="sum":
                    st.session_state["think_summary"]=str(chunk or "")
                elif kind=="done":
                    break
        except Exception as e:
            shown += f"\nâš ï¸ ë™ì‹œ ì‚¬ê³  ì˜ˆì™¸({type(e).__name__}). Mock ì „í™˜."
            ans_holder.markdown(shown)
        if not shown.strip():
            shown="â€» ì—”ì§„ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤. ì„ì‹œ ìš”ì§€: " + " ".join(user_msg.split()[:50])
            ans_holder.markdown(shown)
        st.session_state["messages"].append({"role":"assistant","content":shown})
        if mem_on: add_dialog(sid,"assistant",shown)
        st.session_state["last_logs"]=[l for l in new_logs if l]

st.divider(); st.caption("v3.8 Â· markdown key ì œê±°(í˜¸í™˜) Â· auto-refresh í´ë°± Â· ë™ì‹œ ì‚¬ê³  ìŠ¤íŠ¸ë¦¼")