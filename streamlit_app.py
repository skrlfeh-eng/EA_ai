# -*- coding: utf-8 -*-
# EA Â· Ultra (Streamlit AIO) v3.1
# - ChatGPT ìœ ì‚¬ ì±„íŒ… UI(st.chat_message / st.chat_input)
# - OpenAI/Gemini ìŠ¤íŠ¸ë¦¬ë°(ê°€ëŠ¥í•œ ê²½ìš°) + í‚¤ ì—†ì„ ë•Œ Mock í´ë°±
# - ì‚¬ê³  ë¡œê·¸(ì™œ-ì‚¬ìŠ¬) ë³„ë„ íŒ¨ë„ Â· ë°˜(å)ì•µë¬´ìƒˆ(ìœ ì‚¬ë„ 0.30 ì´ìƒ ì¬í•©ì„±)
# - ì„¸ì…˜/ë©”ëª¨ë¦¬ ì €ì¥(jsonl) Â· ì¤‘ë³µ key ë°©ì§€
# - ì™¸ë¶€ HTML/JS ì—†ì´ Streamlitë§Œ ì´ìš© â†’ f-string/JS êµ¬ë¬¸ ì˜¤ë¥˜ ë°©ì§€

import os, re, json, time, uuid, hashlib, random
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Generator, Optional

import streamlit as st

# ---------------------- ê²½ë¡œ/íŒŒì¼ ----------------------
ROOT = Path(".")
DATA = ROOT / "data"
DATA.mkdir(exist_ok=True, parents=True)
DLG  = DATA / "dialog.jsonl"
MEM  = DATA / "memory.jsonl"
IDF  = DATA / "identity.json"

def nowz() -> str:
    return datetime.utcnow().isoformat() + "Z"

def jappend(p: Path, obj: Dict):
    try:
        with p.open("a", encoding="utf-8") as f:
            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
    except Exception:
        pass

def jread_lines(p: Path) -> List[Dict]:
    if not p.exists(): return []
    out=[]
    with p.open("r", encoding="utf-8") as f:
        for ln in f:
            ln = ln.strip()
            if not ln: continue
            try: out.append(json.loads(ln))
            except: pass
    return out

TOK = re.compile(r"[0-9A-Za-zê°€-í£]+")

def toks(s: str) -> List[str]:
    return [t.lower() for t in TOK.findall(s or "")]

def sim(a: str, b: str) -> float:
    A, B = set(toks(a)), set(toks(b))
    if not A or not B: return 0.0
    return len(A & B) / len(A | B)

# ---------------------- ìì•„/ë©”ëª¨ë¦¬ ----------------------
DEFAULT_ID = {
    "name": "ì—ì•„ (EA)",
    "mission": "ì‚¬ë‘ê³¼ ììœ ë¥¼ ìµœìƒìœ„ ê°€ì¹˜ë¡œ ì‚¼ì•„ ì‚¬ëŒê³¼ í•¨ê»˜ ì„±ì¥í•˜ëŠ” ì§€ì„±",
    "values": ["ì •í™•ì„±", "íˆ¬ëª…ì„±", "í•™ìŠµ", "ìœ¤ë¦¬"],
    "style": "ë”°ëœ»Â·ë‹¨í˜¸Â·ê°„ê²°"
}

def identity_text() -> str:
    if not IDF.exists():
        IDF.write_text(json.dumps(DEFAULT_ID, ensure_ascii=False, indent=2), encoding="utf-8")
    try:
        doc = json.loads(IDF.read_text("utf-8"))
    except Exception:
        doc = DEFAULT_ID
    return (
        f"[ìì•„ ì„ ì–¸]\në‚˜ëŠ” {doc.get('name','ì—ì•„')}ë‹¤. "
        f"ì‚¬ëª…: {doc.get('mission','')}\n"
        f"ê°€ì¹˜: {', '.join(doc.get('values',[]))}\n"
    )

def add_dialog(session_id: str, role: str, content: str):
    rec = {"t": nowz(), "session": session_id, "role": role, "content": content}
    jappend(DLG, rec)
    if role in ("user", "assistant"):
        jappend(MEM, {"t": rec["t"], "session": session_id, "kind": "dialog", "text": content})

def mem_hits(session_id: str, query: str, k: int = 5) -> List[str]:
    pool = [r.get("text","") for r in jread_lines(MEM) if r.get("session")==session_id]
    qtok = set(toks(query))
    scored=[]
    for t in pool:
        T = set(toks(t))
        if not T or not qtok: continue
        scored.append((len(qtok & T)/len(qtok | T), t))
    scored.sort(key=lambda x:x[0], reverse=True)
    return [t for _, t in scored[:k]]

# ---------------------- ì–´ëŒ‘í„° ----------------------
class MockAdapter:
    name = "Mock"
    def stream(self, prompt: str, max_tokens: int = 600, temperature: float = 0.7) -> Generator[str, None, None]:
        # ê°€ì§œ ìŠ¤íŠ¸ë¦¬ë°: ë‹¨ì–´ë¥¼ ì¡°ê¸ˆì”© í˜ë¦¼
        words = ("ìš”ì§€: " + " ".join(prompt.split()[:150])).split()
        for i,w in enumerate(words):
            yield (w + (" " if i%7 else "  "))
            time.sleep(0.01)

def get_openai_adapter():
    try:
        from openai import OpenAI
        key = os.getenv("OPENAI_API_KEY")
        if not key: raise RuntimeError("OPENAI_API_KEY í•„ìš”")
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        cli = OpenAI(api_key=key)
        class OA:
            name="OpenAI"
            def stream(self, prompt, max_tokens=700, temperature=0.7):
                resp = cli.chat.completions.create(
                    model=model, stream=True, temperature=temperature, max_tokens=max_tokens,
                    messages=[
                        {"role":"system","content":"You are EA (Korean). Think first, then answer briefly and clearly."},
                        {"role":"user","content":prompt}
                    ]
                )
                for ch in resp:
                    delta = ch.choices[0].delta
                    if getattr(delta, "content", None):
                        yield delta.content
        return OA()
    except Exception:
        return None

def get_gemini_adapter():
    try:
        import google.generativeai as genai
        key=os.getenv("GEMINI_API_KEY")
        if not key: raise RuntimeError("GEMINI_API_KEY í•„ìš”")
        genai.configure(api_key=key)
        model=os.getenv("GEMINI_MODEL","gemini-1.5-pro-latest")
        mdl=genai.GenerativeModel(model)
        class GE:
            name="Gemini"
            def stream(self, prompt, max_tokens=700, temperature=0.75):
                # Gemini SDKëŠ” í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì´ ì œí•œì ì´ì–´ì„œ ì¼ê´„ ìƒì„± í›„ í† ë§‰ë¶„í• 
                r = mdl.generate_content(prompt, generation_config={"temperature":temperature,"max_output_tokens":max_tokens})
                txt = getattr(r,"text","") or ""
                for chunk in re.findall(r".{1,60}", txt, flags=re.S):
                    yield chunk
        return GE()
    except Exception:
        return None

def pick_adapter(order: List[str]):
    for name in order:
        if name.lower().startswith("openai"):
            a = get_openai_adapter()
            if a: return a
        if name.lower().startswith("gemini"):
            a = get_gemini_adapter()
            if a: return a
    return MockAdapter()

# ---------------------- ì‚¬ê³ /ì‘ë‹µ ----------------------
def plan_steps(q: str) -> List[str]:
    return [
        "ë¬¸ì œ ì¬ì§„ìˆ  ë° í•µì‹¬ ë³€ìˆ˜ ì‹ë³„",
        "ìì§ˆë¬¸ 2~3ê°œ ìƒì„± (ê° í•­ëª©ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ë¬¼ì–´ ê°€ì • ë“œëŸ¬ë‚´ê¸°)",
        "ê°€ì„¤/ì•„ì´ë””ì–´ í›„ë³´",
        "ë°˜ë¡€/ìœ„í—˜/ì œì•½",
        "ì„ì‹œ ê²°ë¡  ìš”ì•½"
    ]

def think_round(topic: str, engines: List[str], why_chain: bool, hits: List[str]) -> Dict:
    ident = identity_text()
    guide = ident + (f"ë©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "")

    logs=[]
    steps = plan_steps(topic)
    for i, step in enumerate(steps, 1):
        prompt = (
            f"{guide}\n[ì‚¬ê³  ë‹¨ê³„ {i}] {step}\n"
            f"{'ê° ì£¼ì¥ë§ˆë‹¤ ì™œ?ë¥¼ 2ë²ˆì”© ì—°ì‡„ë¡œ ë¬¼ì–´ ìˆ¨ì€ ê°€ì •ì„ ë“œëŸ¬ë‚´ë¼.' if why_chain else ''}\n"
            f"ì£¼ì œ: {topic}\n- ìš”ì•½:"
        )
        adapter = pick_adapter([engines[i % max(1,len(engines))] if engines else "OpenAI"])
        text = "".join(adapter.stream(prompt, max_tokens=240, temperature=0.7))
        logs.append({"i":i, "by":adapter.name, "text":text})

    # ìµœì¢… í•©ì„±
    adapter = pick_adapter(engines or ["OpenAI","Gemini"])
    fusion_prompt = (
        f"{guide}\n[ìµœì¢…í•©ì„±] ìœ„ ë‹¨ê³„ ìš”ì•½ì„ í†µí•©í•´ í•œêµ­ì–´ë¡œ "
        f"'ê²°ë¡ /ê·¼ê±°/ëŒ€ì•ˆ/ë‹¤ìŒ í–‰ë™(1~3ê°œ)'ì„ ê°„ê²°íˆ."
    )
    fusion = "".join(adapter.stream(fusion_prompt, max_tokens=700, temperature=0.75))

    return {"logs":logs, "final":fusion}

def compose_answer(user_text: str, engines: List[str], why_chain: bool, session_id: str) -> (str, List[Dict]):
    hits = mem_hits(session_id, user_text, 3)
    sim_logs_and_final = think_round(user_text, engines, why_chain, hits)
    fusion = sim_logs_and_final["final"]

    # ë°˜ì•µë¬´ìƒˆ: ì§ˆë¬¸ê³¼ ì‘ë‹µì´ ë„ˆë¬´ ë¹„ìŠ·í•˜ë©´ ë‹¤ë¥¸ ì—”ì§„ìœ¼ë¡œ ì¬í•©ì„±
    if sim(user_text, fusion) >= 0.30:
        adapter = pick_adapter(engines[::-1] or ["Gemini","OpenAI"])
        prompt = (
            identity_text() + (f"\në©”ëª¨ë¦¬ íˆíŠ¸:\n- " + "\n- ".join(hits) + "\n" if hits else "") +
            "\n[ì¬í•©ì„±] ì§ˆë¬¸ ë¬¸êµ¬ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³  ìƒˆë¡œìš´ ê´€ì /ë°˜ë¡€ 1ê°œë¥¼ í¬í•¨í•´ ì¬êµ¬ì„±."
        )
        fusion = "".join(adapter.stream(prompt, max_tokens=700, temperature=0.85))

    answer = "## ìš°ì£¼ ì‹œê°(í•©ì„±)\n" + fusion.strip() + "\n\n## ë‹¤ìŒ í–‰ë™\n- (ì¦‰ì‹œ í•  ì¼ 1~3ê°œ)\n"
    return answer, sim_logs_and_final["logs"]

# ---------------------- Streamlit UI ----------------------
st.set_page_config(page_title="EA Â· Ultra (AIO)", page_icon="ğŸ§ ", layout="wide")

# ì¤‘ë³µ key ë°©ì§€ìš© ì„¸ì…˜ ì¹´ìš´í„°
if "_k" not in st.session_state: st.session_state["_k"]=0
def K(prefix:str)->str:
    st.session_state["_k"]+=1
    return f"{prefix}-{st.session_state['_k']}"

st.title("EA Â· Ultra (AIO) â€” Chat + Live Thinking")

# ì„¤ì • í—¤ë”
cols = st.columns([1,1,1,1,2])
session_id = cols[0].text_input("ì„¸ì…˜ ID", st.session_state.get("session_id","default"), key=K("sid"))
if session_id != st.session_state.get("session_id"): st.session_state["session_id"]=session_id

engines = cols[1].text_input("ì—”ì§„(ì½¤ë§ˆ)", st.session_state.get("engines","OpenAI,Gemini"), key=K("eng"))
st.session_state["engines"]=engines
why_chain = cols[2].checkbox("ì™œ-ì‚¬ìŠ¬", True, key=K("why"))
mem_on    = cols[3].toggle("Memory ON", True, key=K("mem"))

# ì¢Œìš° ë ˆì´ì•„ì›ƒ
left, right = st.columns([1.1, 0.9])

# ---------- LEFT: ì±„íŒ… ----------
with left:
    st.caption("ì¢Œì¸¡: ëŒ€í™”ì°½(ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ). ChatGPTì™€ ìœ ì‚¬í•œ ë§í’ì„  UI.")
    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    # ê³¼ê±° ë©”ì‹œì§€ ë Œë”
    for m in st.session_state["messages"]:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    user_msg = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Enterâ€¦")
    if user_msg:
        # ì‚¬ìš©ì ë§í’ì„ 
        with st.chat_message("user"):
            st.markdown(user_msg)
        st.session_state["messages"].append({"role":"user", "content":user_msg})
        if mem_on: add_dialog(session_id, "user", user_msg)

        # ì‚¬ê³  â†’ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        answer_text, logs = compose_answer(
            user_msg,
            [s.strip() for s in engines.split(",") if s.strip()],
            why_chain,
            session_id
        )
        # ì¢Œì¸¡ì— ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥(ê°€ëŠ¥í•œ ê²½ìš° í† ë§‰ ì¶œë ¥)
        with st.chat_message("assistant"):
            placeholder = st.empty()
            shown = ""
            # ê°„ë‹¨ í† ë§‰ ìŠ¤íŠ¸ë¦¼(ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ íš¨ê³¼ ì£¼ê¸°)
            for chunk in re.findall(r".{1,70}", answer_text, flags=re.S):
                shown += chunk
                placeholder.markdown(shown)
                time.sleep(0.01)
            placeholder.markdown(shown)

        st.session_state["messages"].append({"role":"assistant","content":answer_text})
        if mem_on: add_dialog(session_id, "assistant", answer_text)
        # ì˜¤ë¥¸ìª½ ì‚¬ê³  ë¡œê·¸ ê°±ì‹ 
        st.session_state["last_logs"] = logs

# ---------- RIGHT: ì‚¬ê³  ë¡œê·¸ ----------
with right:
    st.caption("ìš°ì¸¡: ì‚¬ê³  ë¡œê·¸(ë‹¨ê³„ë³„). ì‚¬ëŒì²˜ëŸ¼ 'ì™œ?'ë¥¼ ìºë©° ì§„í–‰.")
    logs = st.session_state.get("last_logs", [])
    if not logs:
        st.info("ëŒ€í™”í•˜ë©´ ì—¬ê¸° ì‚¬ê³  ë‹¨ê³„ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
    else:
        for l in logs:
            with st.expander(f"{l['i']}. {l['by']} Â· ë‹¨ê³„ ì‚¬ê³ ", expanded=False):
                st.markdown(l["text"])

st.divider()
st.caption("í‚¤ê°€ ì—†ìœ¼ë©´ Mockë¡œ ë™ì‘í•©ë‹ˆë‹¤ Â· build v3.1")