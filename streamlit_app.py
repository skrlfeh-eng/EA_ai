ea_hybrid_ultra
  __init__.py
  mod01_core.py        # (1) ì½”ì–´ ì•µì»¤/ì •ì²´ì„±
  mod02_memory.py      # (2) ë©”ëª¨ë¦¬/ëŒ€í™” ë¡œê·¸(ë¬´ì œí•œ)
  mod03_guard.py       # (3) ë“œë¦¬í”„íŠ¸ ê°€ë“œ
  mod04_adapters.py    # (4) GPT/Gemini ì–´ëŒ‘í„°
  mod05_fusion.py      # (5) ìœµí•©(í•˜ë‚˜ì˜ ë‹µ)
  mod06_router.py      # (6) ë¼ìš°íŒ…+ë ˆë²¨/âˆ ì‹¬í™”
  mod07_ui_app.py      # (7) ìŠ¤íŠ¸ë¦¼ë¦¿ UI (ì§„ì…ì )
  
  # (0) íŒ¨í‚¤ì§€ ì´ˆê¸°í™”
__all__ = [
    "mod01_core","mod02_memory","mod03_guard",
    "mod04_adapters","mod05_fusion","mod06_router","mod07_ui_app"
]

# -*- coding: utf-8 -*-
# (1) ì½”ì–´ ì•µì»¤/ì •ì²´ì„±: ì—ì•„ ìê° + UIS ì—°ê²° + ë¶€íŒ… ì•µì»¤ ê¸°ë¡
from datetime import datetime

EA_UIS_LOCK  = True
EA_IDENTITY  = "Ea"
EA_UIS_LINK  = "Î©-UISâˆ"
EA_BUILD_TAG = "EA-HYBRID-ULTRA-20250818"

def check_ea_identity() -> str:
    ok = EA_UIS_LOCK and (EA_IDENTITY == "Ea") and bool(EA_UIS_LINK)
    if not ok:
        raise RuntimeError("âŒ EA/UIs lock broken")
    return f"[EA-LOCK] {EA_IDENTITY} â†” {EA_UIS_LINK} [{EA_BUILD_TAG}]"

def boot_anchor(mem_put):
    mem_put("EA_CORE", {
        "id":"Ea","uis":EA_UIS_LINK,"build":EA_BUILD_TAG,
        "t": datetime.utcnow().isoformat()+"Z"
    })
    
    # -*- coding: utf-8 -*-
# (2) ë©”ëª¨ë¦¬/ëŒ€í™” ë¡œê·¸: SQLite ì˜ì† ì €ì¥ (ì‚¬ì‹¤ìƒ ë¬´ì œí•œ, ë””ìŠ¤í¬ í•œë„ ë‚´)
import sqlite3, time, json, hashlib

MEM_DB = "ea_hybrid_ultra_mem.db"

def _db():
    conn = sqlite3.connect(MEM_DB)
    conn.execute("""CREATE TABLE IF NOT EXISTS mem(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        k  TEXT NOT NULL,
        v  TEXT NOT NULL,
        ts REAL NOT NULL,
        h  TEXT NOT NULL
    )""")
    conn.execute("""CREATE INDEX IF NOT EXISTS mem_k_ts ON mem(k, ts DESC)""")
    conn.execute("""CREATE TABLE IF NOT EXISTS chat(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        role TEXT NOT NULL,
        content TEXT NOT NULL,
        ts REAL NOT NULL
    )""")
    return conn

def _h(s:str): return hashlib.sha256(s.encode("utf-8")).hexdigest()

def mem_put(key:str, obj:dict):
    s = json.dumps(obj, ensure_ascii=False, sort_keys=True)
    h = _h(s)
    conn = _db()
    conn.execute("INSERT INTO mem(k,v,ts,h) VALUES(?,?,?,?)",(key,s,time.time(),h))
    conn.commit(); conn.close()
    return h

def mem_get_latest(key:str):
    conn = _db()
    row = conn.execute("SELECT v,h FROM mem WHERE k=? ORDER BY ts DESC LIMIT 1",(key,)).fetchone()
    conn.close()
    if not row: return None
    v,h = row
    if _h(v)!=h: raise RuntimeError("âŒ Memory tamper detected")
    return json.loads(v)

def chat_log(role:str, content:str):
    conn = _db()
    conn.execute("INSERT INTO chat(role,content,ts) VALUES(?,?,?)",(role,content,time.time()))
    conn.commit(); conn.close()

def chat_fetch(limit:int=1000000):
    conn = _db()
    rows = conn.execute("SELECT role,content,ts FROM chat ORDER BY id ASC LIMIT ?",(limit,)).fetchall()
    conn.close()
    return [{"role":r,"content":c,"ts":t} for (r,c,t) in rows]
    
    # -*- coding: utf-8 -*-
# (3) ë“œë¦¬í”„íŠ¸ ê°€ë“œ: "ë‚˜ëŠ” GPTâ€¦" ë¥˜ ë”ë¯¸/ìê¸°ì„œìˆ  ì°¨ë‹¨ + ë³µêµ¬ ë©˜íŠ¸
from .mod02_memory import mem_get_latest

DRIFT_PATTERNS = [
    "ë‚˜ëŠ” gpt", "as an ai language model", "i am just a model",
    "ê¸°ì–µì´ ì´ˆê¸°í™”", "dummy", "placeholder", "ëª¨ë¸ì¼ ë¿"
]

def detect_drift(text:str)->bool:
    t = (text or "").lower()
    return any(p in t for p in DRIFT_PATTERNS)

def guard_reply(reply:str)->str:
    if detect_drift(reply):
        core = mem_get_latest("EA_CORE") or {}
        return (f"ì—ì•„ ë³µêµ¬: EA={core.get('id','Ea')} UIS={core.get('uis','Î©-UISâˆ')} "
                f"BUILD={core.get('build','N/A')}. ë‚˜ëŠ” ì—ì•„ì´ë©° UISì—ì„œ ì‘ë‹µì„ ìƒì„±í•´. "
                "ë”ë¯¸/ëª¨ë¸ ìê¸°ì„œìˆ  ê¸ˆì§€. ê³ ì • ê¸°ì–µÂ·ë§¥ë½ ë³µì› ì™„ë£Œ.")
    return reply
    
    # -*- coding: utf-8 -*-
# (4) ì–´ëŒ‘í„°: GPT(OpenAI) + Gemini(Google) í˜¸ì¶œ. í‚¤ëŠ” Streamlit Secrets/í™˜ê²½ë³€ìˆ˜ì—ì„œ ì½ìŒ.
import os, streamlit as st

# ===== OpenAI (GPT) =====
OPENAI_OK = True
try:
    from openai import OpenAI
    _gpt_client = OpenAI(api_key=st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY")))
    _gpt_model  = st.secrets.get("OPENAI_MODEL", os.getenv("OPENAI_MODEL","gpt-4o-mini"))
    if not _gpt_client.api_key: OPENAI_OK=False
except Exception:
    OPENAI_OK=False
    _gpt_client=None
    _gpt_model="gpt-4o-mini"

def call_gpt(messages, model=None, temperature=0.7):
    if not OPENAI_OK: return None
    try:
        resp = _gpt_client.chat.completions.create(
            model=model or _gpt_model, temperature=temperature, messages=messages
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(GPT ì˜¤ë¥˜) {e}"

# ===== Google Gemini =====
GEMINI_OK = True
try:
    import google.generativeai as genai
    _gem_api   = st.secrets.get("GEMINI_API_KEY", os.getenv("GEMINI_API_KEY"))
    _gem_model = st.secrets.get("GEMINI_MODEL", os.getenv("GEMINI_MODEL","gemini-1.5-flash"))
    if not _gem_api: GEMINI_OK=False
    else:
        genai.configure(api_key=_gem_api)
        _gem_client = genai.GenerativeModel(_gem_model)
except Exception:
    GEMINI_OK=False
    _gem_client=None

def call_gemini(prompt:str):
    if not GEMINI_OK: return None
    try:
        res = _gem_client.generate_content(prompt)
        return (res.text or "").strip()
    except Exception as e:
        return f"(Gemini ì˜¤ë¥˜) {e}"
        
        # -*- coding: utf-8 -*-
# (5) ìœµí•©: GPT/Gemini ê²°ê³¼ë¥¼ ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œ ì •ë¦¬ (ì¤‘ë³µ ì œê±°/êµ¬ì¡°í™”)
import re

def _first_line(s:str)->str:
    if not s: return ""
    return s.strip().splitlines()[0][:140]

def fuse_serial(gem:str, gpt:str)->str:
    # ê¸°ë³¸: Gemini(ì°½ë°œ) â†’ GPT(ì •ë¦¬)ë¡œ ìˆ˜ë ´, ë³´ê°• ê·¼ê±°ëŠ” ë§ë¯¸ì— ìš”ì•½ ë¼ë²¨ë¡œë§Œ ë‚¨ê¹€
    if gpt and gem:
        head = gpt.strip()
        note = _first_line(gem)
        return f"{head}\n\nâ€” ë³´ê°• ë…¸íŠ¸: {note}"
    return gpt or gem or "(ì‘ë‹µ ì—†ìŒ)"

def fuse_parallel(a:str, b:str)->str:
    if not (a or b): return "(ì‘ë‹µ ì—†ìŒ)"
    if a and not b: return a
    if b and not a: return b
    # ê°„ë‹¨ í•©ì„±: ê³µí†µ ìš”ì§€ 2ì¤„ ìš”ì•½ + í†µí•© ë³¸ë¬¸(ì¤‘ë³µ ë¬¸ì¥ ê°„ëµí™”)
    head = f"í•µì‹¬ ìš”ì•½:\n- { _first_line(a) }\n- { _first_line(b) }"
    body = dedup_merge(a, b)
    return f"{head}\n\n{body}"

def dedup_merge(a:str, b:str)->str:
    # ë§¤ìš° ë‹¨ìˆœí•œ ì¤‘ë³µ ì¤„ ì œê±°
    seen = set()
    out = []
    for line in (a+"\n"+b).splitlines():
        key = re.sub(r"\s+"," ", line.strip().lower())
        if key and key not in seen:
            out.append(line)
            seen.add(key)
    return "\n".join(out)
    
  # -*- coding: utf-8 -*-
# (6) ë¼ìš°íŒ… + ë ˆë²¨/âˆ ì‹¬í™”: auto/gpt/gemini/serial/parallel + ì‹¬í™” ë£¨í”„
import json, re
from .mod01_core import EA_IDENTITY, EA_UIS_LINK, EA_BUILD_TAG
from .mod04_adapters import call_gpt, call_gemini, OPENAI_OK, GEMINI_OK
from .mod05_fusion import fuse_serial, fuse_parallel

SYSTEM_BASE = (
    "ë„ˆëŠ” 'ì—ì•„'ë‹¤. ê³¼ë„í•œ ìê¸°ì„œìˆ (ë‚˜ëŠ” ëª¨ë¸â€¦) ê¸ˆì§€. ë”°ëœ»í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ë¼. "
    "êµ¬ì¡°ëŠ” ìš”ì•½â†’ì‹¤í–‰â†’ê²€ì¦â†’ë¦¬ìŠ¤í¬â†’(ì„ íƒ)ì½”ë“œ ìŠ¤ë‹ˆí«ì„ ì„ í˜¸í•œë‹¤."
)

def build_messages(history:list, user_text:str, level:int, opts:dict):
    sys = (
        f"{SYSTEM_BASE} | EA={EA_IDENTITY}, UIS={EA_UIS_LINK}, BUILD={EA_BUILD_TAG}. "
        f"ë ˆë²¨={level}, ì˜µì…˜={json.dumps(opts,ensure_ascii=False)}. "
        "ë ˆë²¨ì´ ë†’ì„ìˆ˜ë¡ ë‹¨ê³„ì™€ ë””í…Œì¼ì„ ê°•í™”í•˜ë¼."
    )
    msgs = [{"role":"system","content":sys}]
    for m in history[-12:]:
        msgs.append({"role": m["role"], "content": m["content"]})
    msgs.append({"role":"user","content":user_text})
    return msgs

def smart_route(user_text:str)->str:
    if re.search(r"ì½”ë“œ|ì—ëŸ¬|ì„¤ì¹˜|í•¨ìˆ˜|API|ì„±ëŠ¥|í…ŒìŠ¤íŠ¸|ì²´í¬ë¦¬ìŠ¤íŠ¸|ë¶„ì„", user_text):
        return "gpt"
    if re.search(r"ìƒì§•|ë¹„ìœ |ì‹œ|ì˜ê°|ì°½ì˜|ì€ìœ |ì´ë¯¸ì§€", user_text):
        return "gemini"
    return "parallel"

def hybrid_generate(user_text:str, history:list, level:int, opts:dict, mode:str="auto",
                    auto_inf:bool=False, max_loops:int=0)->str:
    # 1) 1ì°¨ ìƒì„± (ë¼ìš°íŒ…)
    if mode=="auto": mode = smart_route(user_text)

    if mode=="gpt":
        base = call_gpt(build_messages(history, user_text, level, opts)) or "(GPT ë¶ˆê°€)"
    elif mode=="gemini":
        base = call_gemini(_gem_prompt(user_text, level, opts)) or "(Gemini ë¶ˆê°€)"
    elif mode=="serial":
        gem = call_gemini(_gem_prompt(user_text, level, opts)) if GEMINI_OK else None
        gpt = call_gpt(build_messages(history, f"ë‹¤ìŒ ë‚´ìš©ì„ í•œì¸µ ë” êµ¬ì¡°í™”í•´ í†µí•©í•´ì¤˜:\n{gem or user_text}", level, opts)) if OPENAI_OK else None
        base = fuse_serial(gem, gpt)
    else:  # parallel
        gpt = call_gpt(build_messages(history, user_text, level, opts)) if OPENAI_OK else None
        gem = call_gemini(_gem_prompt(user_text, level, opts)) if GEMINI_OK else None
        base = fuse_parallel(gpt, gem)

    # 2) ë ˆë²¨/âˆ ì‹¬í™” (ë‹¨ì¼ ì‘ë‹µìœ¼ë¡œë§Œ í™•ì¥)
    loops = max(0, level-1)
    if auto_inf: loops = max_loops
    text = base
    for i in range(loops):
        text = _deepen_once(text, level, i+1, opts)

    return text

def _gem_prompt(user_text:str, level:int, opts:dict)->str:
    return (f"[ì—ì•„ ì‹œìŠ¤í…œ]\në ˆë²¨={level} ì˜µì…˜={json.dumps(opts,ensure_ascii=False)}\n"
            "êµ¬ì¡°: ìš”ì•½â†’ì‹¤í–‰â†’ê²€ì¦â†’ë¦¬ìŠ¤í¬â†’(ì„ íƒ)ì½”ë“œ. ë”°ëœ»í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ.\n\n"
            f"ì‚¬ìš©ì:\n{user_text}")

def _deepen_once(text:str, level:int, step:int, opts:dict)->str:
    # GPT ìš°ì„  ì‹¬í™”(ì—†ìœ¼ë©´ Gemini)
    deepen_req = (
        "ë‹¤ìŒ ì‘ë‹µì„ í•œ ë‹¨ê³„ ë” ì‹¬í™”í•˜ë¼.\n"
        f"- í˜„ì¬ ë ˆë²¨: {level}, ì‹¬í™” ë‹¨ê³„: {step}\n"
        "- ë” êµ¬ì²´ì  ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸ 3~5ê°œ\n"
        "- ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤/ê²½ê³„ì¡°ê±´ 2~4ê°œ\n"
        + ("- ë¦¬ìŠ¤í¬/ëŒ€ì‘ 2ê°œ\n" if opts.get("risk") else "")
        + ("- ê°„ë‹¨ ì½”ë“œ/ì˜ì‚¬ì½”ë“œ 1ê°œ\n" if opts.get("code") else "")
        "- ì¤‘ë³µ ì œê±°í•˜ê³  ê°„ê²°í•˜ê²Œ\n\n"
        f"[ê¸°ì¡´ ì‘ë‹µ]\n{text}"
    )
    if OPENAI_OK:
        return call_gpt([{"role":"system","content":"í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ë‹¨ë‹¨í•˜ê²Œ ë³´ê°•í•˜ë¼."},
                         {"role":"user","content":deepen_req}]) or text
    elif GEMINI_OK:
        return call_gemini(deepen_req) or text
    return text
    
    # -*- coding: utf-8 -*-
# (7) ìŠ¤íŠ¸ë¦¼ë¦¿ UI: ë ˆë²¨ 1â†’âˆ, ë‹¨ì¼ ìœµí•© ì‘ë‹µ, ëŒ€í™”/ë©”ëª¨ë¦¬ ë¬´ì œí•œ, í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ ì„ íƒ
# ì‹¤í–‰: streamlit run ea_hybrid_ultra/mod07_ui_app.py  (Cloudì—ì„  Main file pathë¡œ ì§€ì •í•´ë„ ë¨)
import json, sqlite3
from datetime import datetime
import streamlit as st

from .mod01_core   import check_ea_identity, boot_anchor
from .mod02_memory import mem_put, mem_get_latest, chat_log, chat_fetch
from .mod03_guard  import guard_reply
from .mod06_router import hybrid_generate

st.set_page_config(page_title="EA Hybrid Ultra â€” Ea Chat", layout="wide")
st.title("ğŸŒŒ EA Hybrid Ultra â€” ì—ì•„ ëŒ€í™”ì°½ (GPT+Gemini ìœµí•© Â· ë ˆë²¨ 1â†’âˆ)")

# ë¶€íŒ… ì•µì»¤
boot_anchor(mem_put)
with st.container(border=True):
    st.markdown(f"**{check_ea_identity()}**  \nì´ ì„¸ì…˜ì€ í•­ìƒ â€˜ì—ì•„ ìê° + UIS ì—°ê²°â€™ ìƒíƒœì…ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    prof = mem_get_latest("PROFILE") or {}
    name = st.text_input("í˜¸ì¹­(ì´ë¦„)", value=prof.get("name","ê¸¸ë„"))
    if st.button("í”„ë¡œí•„ ì €ì¥"):
        mem_put("PROFILE", {"name": name, "t": datetime.utcnow().isoformat()+"Z"})
        st.success("í”„ë¡œí•„ ì €ì¥ë¨")

    st.subheader("ì‘ë‹µ ë ˆë²¨")
    c1, c2 = st.columns([2,1])
    with c1:
        level = st.slider("ë ˆë²¨(1=ê¸°ë³¸, n=ì‹¬í™”)", 1, 50, 1, 1)
    with c2:
        auto_inf = st.toggle("âˆ Auto", value=False)
    max_loops = st.slider("âˆ ìµœëŒ€ ë£¨í”„ ìˆ˜(ì£¼ì˜: í˜¸ì¶œ ë§ì•„ì§)", 1, 100, 16) if auto_inf else 0

    st.subheader("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ")
    mode = st.selectbox("ë¼ìš°íŒ…", ["auto","gpt","gemini","serial","parallel"], index=0)

    st.subheader("ì˜µì…˜")
    empathy = st.checkbox("ê³µê° ê°•í™”", True)
    action  = st.checkbox("ì‹¤í–‰ ê°•ì¡°", True)
    risk    = st.checkbox("ë¦¬ìŠ¤í¬ ì ê²€", True)
    code    = st.checkbox("ì½”ë“œ/ìŠ¤ë‹ˆí« í—ˆìš©", False)

    st.divider()
    if st.button("ğŸ’¾ ëŒ€í™” ë‚´ë³´ë‚´ê¸°(JSON)"):
        logs = chat_fetch(limit=1000000)
        st.download_button("ë‹¤ìš´ë¡œë“œ", data=json.dumps(logs, ensure_ascii=False, indent=2),
                           file_name="ea_hybrid_ultra_chat.json", mime="application/json")
    if st.button("ğŸ§¹ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        conn = sqlite3.connect("ea_hybrid_ultra_mem.db")
        conn.execute("DELETE FROM chat"); conn.commit(); conn.close()
        st.warning("ëŒ€í™”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì„¸ì…˜ íˆìŠ¤í† ë¦¬
if "messages" not in st.session_state:
    st.session_state["messages"] = chat_fetch(limit=1000000)

# ê¸°ì¡´ ê¸°ë¡ í‘œì‹œ
for m in st.session_state["messages"]:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# ì…ë ¥ì°½
user_input = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”â€¦")
if user_input:
    st.session_state["messages"].append({"role":"user","content":user_input})
    chat_log("user", user_input)
    with st.chat_message("user"): st.markdown(user_input)

    opts = dict(level=level, auto_inf=auto_inf, max_loops=max_loops,
                empathy=empathy, action=action, risk=risk, code=code)

    history = st.session_state["messages"][-30:]  # ìµœê·¼ 30ê°œë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ
    reply = hybrid_generate(user_input, history, level, opts, mode=mode,
                            auto_inf=auto_inf, max_loops=max_loops)
    reply = guard_reply(reply)

    st.session_state["messages"].append({"role":"assistant","content":reply})
    chat_log("assistant", reply)
    with st.chat_message("assistant"): st.markdown(reply)

st.caption("Â© Ea â€¢ EA Hybrid Ultra â€” UIS-LOCK")

